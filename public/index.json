[
{
	"uri": "https://master.d2fvrafk9v089j.amplifyapp.com/01-hpc-overview.html",
	"title": "AWS HPC Overview",
	"tags": [],
	"description": "",
	"content": "Select a workshop from the left panel or just click and explore the workshops highlighted below.\nThis series of workshops is designed to get you familiar with the concepts and best practices to understand AWS components that help to build an HPC cluster and run your HPC workloads on HPC efficiently.\nAfter an optional introduction and setup stage, we will walk you through the following labs:\n  AWS Getting started to familiarize yourself with the AWS Cloud.\n  Introduction to AWS ParallelCluster introduces you to run HPC workloads in the cloud.\n  Advanced AWS ParallelCluster will drive you through HPC focused services.\n  We recommend you take these labs in the order presented as some dependencies exists between them, but feel free to change the order based on your comfort level.\n"
},
{
	"uri": "https://master.d2fvrafk9v089j.amplifyapp.com/04-amazon-fsx-for-lustre/01-install-pc.html",
	"title": "a. Install AWS ParallelCluster",
	"tags": ["tutorial", "install", "ParallelCluster"],
	"description": "",
	"content": " You can skip the following step you already have ParallelCluster installed on your AWS Cloud9 instance.\n To return to the AWS cloud9 instance, click on the AWS logo in the upper left corner. Search for, and select, cloud9 from the console search bar. Click on “open IDE” for the cloud9 instance set up previously and wait for the IDE to open. This can take a few moments as AWS Cloud9 will “stop” and “restart” the instance so that the user does not pay compute charges when no longer using the cloud9 IDE.\nInstall AWS ParallelCluster In this section, AWS ParallelCluster is installed on the AWS Cloud9 instance. Python and PIP (Python package management tool) are already installed in the Cloud9 environment. Use the pip install command to install AWS ParallelCluster:\npip-3.6 install aws-parallelcluster -U --user "
},
{
	"uri": "https://master.d2fvrafk9v089j.amplifyapp.com/05-aws-batch/01-requirement.html",
	"title": "a. Workshop Initial Setup",
	"tags": ["tutorial", "install", "AWS", "Batch"],
	"description": "",
	"content": "An initial setup must be conducted by the main account owner. During this setup we will:\n Create an S3 bucket to store simulations outputs. Define an IAM role for ECS tasks to write the simulations output in that bucket. Build an EC2 role to access the S3 bucket where Nvidia drivers are stored.  The steps described above will be conducted using a CloudFormation script and executed through the commands below. They must be executed on the main account using admin credentials using a terminal on Linux, OSX or WSL:\ncurl https://s3.amazonaws.com/av-workshop/carla_0.9.5_ami_generation.zip \\  -o carla_0.9.5_ami_generation.zip unzip carla_0.9.5_ami_generation.zip pushd carla_0.9.5_ami_generation/utils bash cfn_pre-requisites.sh popd The script will wait until the base infrastructure is ready then will output the EC2 instance role needed to access the Nvidia drivers on S3.\nPlease keep the following information at hand for the next steps of the lab: EC2 Role ID, ECS Task Role ID and S3 Bucket.\n "
},
{
	"uri": "https://master.d2fvrafk9v089j.amplifyapp.com/02-aws-getting-started.html",
	"title": "Getting Started on the Cloud",
	"tags": ["HPC", "Introduction", "EC2", "Optional"],
	"description": "",
	"content": "Overview During this workshop we will drive you through the setup of your AWS account to run HPC your own workloads. You will learn to navigate the AWS Console, access relevant services and learn to deploy a basic infrastructure.\n Log onto the AWS Management Console and explore it. Launch a cloud based IDE (AWS Cloud9), this will be your portal to the AWS CLI. Create an EC2 instance and SSH into it. Upload and Download a file using object storage (Amazon S3) and the AWS CLI. Look at IAM and attach an IAM role to an EC2 instance.  "
},
{
	"uri": "https://master.d2fvrafk9v089j.amplifyapp.com/06-running-gatk-on-aws-parallelcluster/02-configure-gatk-pc.html",
	"title": "a. Configure AWS ParallelCluster for GATK",
	"tags": ["tutorial", "configure", "initialize", "ParallelCluster"],
	"description": "",
	"content": " For this lab we assume that you have an AWS Cloud9 IDE ready and are familiar with AWS ParallelCluster. If this is not the case, please run through the first half of Getting Started with AWS lab.\n To return to the AWS Cloud9 instance, click on the AWS logo in the upper left corner. Search for, and select, Cloud9 from the console search bar. Click on \u0026ldquo;open IDE\u0026rdquo; for the Cloud9 instance set up previously and wait for the IDE to open. This can take a few moments as AWS Cloud9 will \u0026ldquo;stop\u0026rdquo; and \u0026ldquo;restart\u0026rdquo; the instance so that the user does not pay compute charges when no longer using the Cloud9 IDE.\nPlease double check you are using the correct key-pair\n Configuring Your HPC GATK cluster We will be generating a cluster with the following settings:\n  Head-node and compute nodes will be m5.xlarge instances. Change the instance type if you like but you may run into EC2 limits that may prevent you to create some or too many instances.\n  The cluster will have a minimum queue size of 1 and a maximum queue size of 4.\n  We use an AMI that has been preloaded with GATK, BWA, samtools, Picard and other packages used in this example. This AMI is made available for you to save time. To learn about how to create your own custom AMI, see Creating a Custom AMI.\n  We will mount an EBS snapshot that has a copy of the [Broad genomics reference data] (https://registry.opendata.aws/broad-references/). This snapshot also has some test data for GATK for running this example. This snapshot will be used to create a volume that is then mounted on the compute nodes on /genomics. Please keep in mind that the /home directory is shared on NFS as well.\n  We use SLURM as a job scheduler but there are other options you may consider in the future such as SGE.\n  For more details about the configuration options of AWS ParallelCluster please review the user guide.\n To create a configration file you can paste the following commands in your terminal:\n# generate a new key-pair aws ec2 create-key-pair --key-name gatk-lab-your-key --query KeyMaterial --output text \u0026gt; ~/.ssh/gatk-lab-key chmod 600 ~/.ssh/gatk-lab-key IFACE=$(curl --silent http://169.254.169.254/latest/meta-data/network/interfaces/macs/) SUBNET_ID=$(curl --silent http://169.254.169.254/latest/meta-data/network/interfaces/macs/${IFACE}/subnet-id) VPC_ID=$(curl --silent http://169.254.169.254/latest/meta-data/network/interfaces/macs/${IFACE}/vpc-id) AZ=$(curl http://169.254.169.254/latest/meta-data/placement/availability-zone) REGION=${AZ::-1} mkdir -p ~/environment cd ~/environment cat \u0026gt; gatk-config.conf \u0026lt;\u0026lt; EOF [aws] aws_region_name = us-east-1 [global] update_check = true sanity_check = true cluster_template = default [aliases] ssh = ssh {CFN_USER}@{MASTER_IP} {ARGS} [cluster default] base_os = alinux custom_ami = ami-04b8792c54afb559e # specific to region vpc_settings = public scheduler = slurm key_name = gatk-lab-key compute_instance_type = m5.xlarge master_instance_type = m5.xlarge compute_root_volume_size = 50 master_root_volume_size = 50 ebs_settings = genomes scaling_settings = GATK-ASG initial_queue_size = 1 max_queue_size = 4 maintain_initial_size = false extra_json = { \u0026#34;cluster\u0026#34; : { \u0026#34;cfn_scheduler_slots\u0026#34; : \u0026#34;cores\u0026#34; } } [vpc public] vpc_id = ${VPC_ID} master_subnet_id = ${SUBNET_ID} [ebs genomes] shared_dir = genomes ebs_snapshot_id = snap-00870c3aa41831ffb #modify volume_type = gp2 volume_size = 1024 [scaling GATK-ASG] scaledown_idletime = 5 Now you are ready to launch a cluster, proceed to the next step.\n"
},
{
	"uri": "https://master.d2fvrafk9v089j.amplifyapp.com/03-hpc-aws-parallelcluster-workshop/02-install-pc.html",
	"title": "a. Install AWS ParallelCluster",
	"tags": ["tutorial", "install", "ParallelCluster"],
	"description": "",
	"content": " For this lab one will assume that you have an AWS Cloud9 IDE ready. If this is not the case, please run through the first half of Getting Started with AWS lab.\n To return to the AWS Cloud9 instance, click on the AWS logo in the upper left corner. Search for, and select, Cloud9 from the console search bar. Click on \u0026ldquo;open IDE\u0026rdquo; for the Cloud9 instance set up previously and wait for the IDE to open. This can take a few moments as AWS Cloud9 will \u0026ldquo;stop\u0026rdquo; and \u0026ldquo;restart\u0026rdquo; the instance so that the user does not pay compute charges when no longer using the Cloud9 IDE.\nInstall AWS ParallelCluster In this section, AWS ParallelCluster is installed on the AWS Cloud9 instance. Python and PIP (Python package management tool) are already installed in the Cloud9 environment. Use the pip install command to install AWS ParallelCluster:\npip install aws-parallelcluster -U --user "
},
{
	"uri": "https://master.d2fvrafk9v089j.amplifyapp.com/05-aws-batch/02-build-ami-packer.html",
	"title": "b. Build Your AMI with Packer",
	"tags": ["tutorial", "install", "AWS", "batch", "packer"],
	"description": "",
	"content": "During this step we will conduct an initial setup for the for the demo. We will first create an AMI and the container then upload this container image to Amazon Elastic Container Registry (ECR) for later use with AWS Batch.\nYou will need to execute the following scripts in your terminal. You AWS CLI must be configured and you must have admin access on the account.\nThe following script will build an Elastic Container Service (ECS) compatible AMI with CARLA:\nINSTANCE_ROLE=$(aws cloudformation describe-stacks --stack-name PrepAVWorkshop --output text --query \u0026#39;Stacks[0].Outputs[?OutputKey == `S3RoleARN`].OutputValue\u0026#39;) pushd carla_0.9.5_ami_generation bash build.sh $INSTANCE_ROLE The building process will be handled by Packer from HashiCorp. It will involve multiple reboots to update to the latest driver and kernel modules. We would recommend for you to check the content of the file image.json while the AMI is being built.\n"
},
{
	"uri": "https://master.d2fvrafk9v089j.amplifyapp.com/04-amazon-fsx-for-lustre/02-configure-pc-fsx.html",
	"title": "b. Create your HPC Cluster",
	"tags": ["configuration", "FSx", "ParallelCluster"],
	"description": "",
	"content": "Now we will configure AWS ParallelCluster with a configuration for including parameters to use Amazon FSx for Lustre. If not already the case, it is recommended that your go through the first AWS ParallelCluster lab before doing that one. If you feel comfortable, feel free to continue.\nCreate an Amazon S3 Bucket and Upload Files We will first create an S3 bucket and upload a file so you can retrieve it using Amazon FSx for Lustre.\nGo to your AWS Cloud9 instance and open a terminal. Then run the commands below to create a new Amazon S3 bucket. We will also store some files on this bucket and first retrieve from MatrixMarket and a velocity model from the Society of Exploration Geophysicists for that matter.\n# generate a uniqe postfix BUCKET_POSTFIX=$(uuidgen --random | cut -d\u0026#39;-\u0026#39; -f1) echo \u0026#34;Your bucket name will be mybucket-${BUCKET_POSTFIX}\u0026#34; aws s3 mb s3://mybucket-${BUCKET_POSTFIX} # retrieve local copies wget ftp://math.nist.gov/pub/MatrixMarket2/misc/cylshell/s3dkq4m2.mtx.gz wget http://s3.amazonaws.com/open.source.geoscience/open_data/seg_eage_salt/SEG_C3NA_Velocity.sgy # upload to your bucket aws s3 cp s3dkq4m2.mtx.gz s3://mybucket-${BUCKET_POSTFIX}/s3dkq4m2.mtx.gz aws s3 cp SEG_C3NA_Velocity.sgy s3://mybucket-${BUCKET_POSTFIX}/SEG_C3NA_Velocity.sgy # delete local copies rm s3dkq4m2.mtx.gz rm SEG_C3NA_Velocity.sgy Before going to the next step you can check the content of your bucket using the AWS CLI or the AWS console. Now let's build our AWS ParallelCluster configuration.\nGenerating a Cluster Configuration for Amazon FSx for Lustre We will assume that you already are familiar with AWS ParallelCluster and the process of bootstrapping a cluster. We will take few shortcuts for that reason.\nWe will generate a new key-pair and new default AWS ParallelCluster configuration with the following settings:\n Request a Lustre partition of 3.6 TB and set the Amazon S3 bucket created previously as the import and export path. Head-node and compute nodes will be c4.xlarge instances. Change the instance type if you like but you may run into EC2 limits that may prevent you to create some or too many instances. We use a placement group to maximize the bandwidth between instances and reduce the latency. More details. The cluster will have 0 compute nodes when starting, the minimum size is set to 0 and the maximum size of the cluster is set to 8 instances. We are using auto-scaling groups that will grow and shrink between the min and max limits based on the cluster utilization and job queue backlog. A GP2 Amazon EBS volume will be attached to the head-node then shared through NFS to be mounted by the compute nodes on /shared. It is generally a good location to store applications or scripts. Please keep in mind that the /home directory is shared on NFS as well. SLURM will be used as a job scheduler but there are other options you may consider in the future such as SGE.  For more details about the configuration options of AWS ParallelCluster please review the user guide. We will highly recommend to also look in more details at parameters dedicated to Amazon FSx for Lustre.\n If you are using a different terminal than above, please make sure that the Amazon S3 bucket name is correct.\n Paste the following commands in your terminal:\n# generate a new keypair, remove those lines if you want to use the previous one aws ec2 create-key-pair --key-name lab-4-your-key --query KeyMaterial --output text \u0026gt; ~/.ssh/lab-4-key chmod 600 ~/.ssh/lab-4-key # create the cluster configuration IFACE=$(curl --silent http://169.254.169.254/latest/meta-data/network/interfaces/macs/) SUBNET_ID=$(curl --silent http://169.254.169.254/latest/meta-data/network/interfaces/macs/${IFACE}/subnet-id) VPC_ID=$(curl --silent http://169.254.169.254/latest/meta-data/network/interfaces/macs/${IFACE}/vpc-id) AZ=$(curl http://169.254.169.254/latest/meta-data/placement/availability-zone) REGION=${AZ::-1} mkdir -p ~/.parallelcluster cat \u0026gt; ~/.parallelcluster/config \u0026lt;\u0026lt; EOF [aws] aws_region_name = ${REGION} [global] cluster_template = default update_check = false sanity_check = true [cluster default] key_name = lab-4-your-key vpc_settings = public ebs_settings = myebs fsx_settings = myfsx compute_instance_type = c4.xlarge master_instance_type = c4.xlarge cluster_type = ondemand placement_group = DYNAMIC placement = compute max_queue_size = 8 initial_queue_size = 0 disable_hyperthreading = true scheduler = slurm [vpc public] vpc_id = ${VPC_ID} master_subnet_id = ${SUBNET_ID} [ebs myebs] shared_dir = /shared volume_type = gp2 volume_size = 20 [fsx myfsx] shared_dir = /lustre storage_capacity = 3600 import_path = s3://mybucket-${BUCKET_POSTFIX} [aliases] ssh = ssh {CFN_USER}@{MASTER_IP} {ARGS} EOF If you like check the content of your configuration file.\ncat ~/.parallelcluster/config Now we are ready to create a cluster.\nGenerating a Cluster Configuration for Amazon FSx for Lustre Create the cluster with the following command.\npcluster create my-fsx-cluster It will take a bit longer to create the cluster than for the previous lab as AWS ParallelCluster will generate additional resources for Amazon FSx for Lustre.\nConnect to Your Cluster Once created, let's connect to our cluster.\npcluster ssh my-fsx-cluster -i ~/.ssh/lab-4-key We will then take a deeper look at our Lustre file system.\n"
},
{
	"uri": "https://master.d2fvrafk9v089j.amplifyapp.com/03-hpc-aws-parallelcluster-workshop.html",
	"title": "AWS ParallelCluster",
	"tags": ["HPC", "Overview"],
	"description": "",
	"content": "AWS ParallelCluster is an AWS-supported open source cluster management tool that helps you to deploy and manage High Performance Computing (HPC) clusters in the AWS Cloud. Built on the open source CfnCluster project, AWS ParallelCluster enables you to quickly build an HPC compute environment in AWS. It automatically sets up the required compute resources and shared filesystem. You can use AWS ParallelCluster with a variety of batch schedulers, such as AWS Batch, SGE, Torque, and Slurm. AWS ParallelCluster facilitates quick start proof of concept deployments and production deployments. You can also build higher level workflows, such as a genomics portal that automates an entire DNA sequencing workflow, on top of AWS ParallelCluster.\nFor this lab one will assume that you have an AWS Cloud9 IDE ready. If this is not the case, please run through the first half of Getting Started with AWS lab.\n In this lab we will introduce you to AWS ParallelCluster and how to run your HPC jobs in AWS as you would on-premises. The steps we will be going through are the following:\n Install and configure ParallelCluster on your AWS Cloud9 IDE. Create your first cluster. Submit a sample job and check what is happening in the background. Delete the cluster.  "
},
{
	"uri": "https://master.d2fvrafk9v089j.amplifyapp.com/06-running-gatk-on-aws-parallelcluster.html",
	"title": "Running GATK on AWS ParallelCluster",
	"tags": ["GATK", "Overview"],
	"description": "",
	"content": "This lab will help you to launch an AWS ParallelCluster configured to run Genome Analysis Toolkit (GATK) . GATK, developed at the BROAD institute, offers a wide variety of tools with a primary focus on variant discovery and genotyping. The AMI used as a basis for this cluster contains GATK related tools such as bwa, Samtools, gatk4 and so on. The GATK public database and other testing data have been stored on a snapshot that is mounted on the shared /genomics directory when the cluster is created.\nFor this lab we assume that you have an AWS Cloud9 IDE ready. If you do not, please run through the first half of Getting Started with AWS lab. We also assume that you have gone through the general tutorial on AWS ParallelCluster and are familiar with configuring and building a cluster.\n In this lab we build on AWS ParallelCluster to describe how to create a cluster for running GATK.\n Configure a GATK cluster on your AWS Cloud9 IDE, mounting a shared volume of genomics data. Create your GATK cluster. Submit a sample job. Delete the cluster.  "
},
{
	"uri": "https://master.d2fvrafk9v089j.amplifyapp.com/02-aws-getting-started/02-requirement_notes.html",
	"title": "a. Pre-Requisites",
	"tags": ["tutorial", "Pre-Requisite", "ec2"],
	"description": "",
	"content": "The following requirements will be necessary to follow this guide.\n A laptop with an internet connection running Microsoft Windows, Mac OS X, or Linux. An Internet browser such as Chrome, Firefox, Safari, Opera or Edge. Familiarity with common Linux commands.  Should you have any question when running this workshop as a group or on your own, do not hesitate to talk to you coordinator or contact us at AWS HPC\nPlease take note that this tutorial will contain some codes to run a terminal that you can copy using the button mentioned below.\n"
},
{
	"uri": "https://master.d2fvrafk9v089j.amplifyapp.com/03-hpc-aws-parallelcluster-workshop/03-initialize-pc.html",
	"title": "b. Initialization",
	"tags": ["tutorial", "initialize", "ParallelCluster"],
	"description": "",
	"content": "Next, you will configure AWS ParallelCluster.\nTo configure AWS ParallelCluster, you could use the command pcluster configure in a terminal and provide the requested information such as the AWS Region, Scheduler and EC2 Instance Type. However, today we will take a shortcut by creating a basic configuration file, then customizing this file to include HPC specific options.\nThe commands below generate a new keypair, query the EC2 metadata to get the Subnet ID, VPC ID, and lastly write a config to ~/.parallelcluster/config. You can always edit this config file to add and change configuration options.\n# generate a new key-pair aws ec2 create-key-pair --key-name lab-3-your-key --query KeyMaterial --output text \u0026gt; ~/.ssh/lab-3-key chmod 600 ~/.ssh/lab-3-key IFACE=$(curl --silent http://169.254.169.254/latest/meta-data/network/interfaces/macs/) SUBNET_ID=$(curl --silent http://169.254.169.254/latest/meta-data/network/interfaces/macs/${IFACE}/subnet-id) VPC_ID=$(curl --silent http://169.254.169.254/latest/meta-data/network/interfaces/macs/${IFACE}/vpc-id) mkdir -p ~/.parallelcluster cat \u0026gt; ~/.parallelcluster/config \u0026lt;\u0026lt; EOF [aws] aws_region_name = us-east-1 [cluster default] key_name = lab-3-your-key vpc_settings = public [vpc public] vpc_id = ${VPC_ID} master_subnet_id = ${SUBNET_ID} [global] cluster_template = default update_check = false sanity_check = true [aliases] ssh = ssh {CFN_USER}@{MASTER_IP} {ARGS} EOF Now check the content of this file using:\ncat ~/.parallelcluster/config You now have a configuration file allowing you to create a simple cluster with the minimum required information. A default configuration file is good to have for testing purposes. Next, we build a configuration to generate an optimized cluster to run typical \u0026ldquo;tightly coupled\u0026rdquo; HPC applications.\n"
},
{
	"uri": "https://master.d2fvrafk9v089j.amplifyapp.com/04-amazon-fsx-for-lustre/03-check-fs.html",
	"title": "c. File System Examination",
	"tags": ["tutorial", "HSM", "FSx"],
	"description": "",
	"content": "We will now take a look at our file system, it's structure and content.\nLet's take a look at our file system and list all the files present.\ntime lfs find /lustre You should see something similar as in the image below.\nLet's take a deeper dive at our files and see how much they weight by listing the content of the /lustre directory.\nls -lh /lustre And you should see something like that:\nNow we will look at how much data is stored on the Lustre partition for the Metadata Target (MDT) and Object Storage Targets (OSTs) as follows.\ntime lfs df -h The result should be comparable to the one shown below.\nCan you identify a discrepancy between\n The size of the files ~ 468 MB The data actually stored on the Lustre partition ~ 13.5 MB of content and 5.8 MB of metadata.  This is due to the Lazy loading functionality of Amazon FSx for Lustre when it is linked to an Amazon S3 bucket. Only the metadata of the objects is retrieved on the MDT, the actual bytes or content of the files will be copied at first read.\n"
},
{
	"uri": "https://master.d2fvrafk9v089j.amplifyapp.com/05-aws-batch/03-create-docker-repo.html",
	"title": "c. Repository &amp; Container",
	"tags": ["tutorial", "install", "AWS", "batch", "packer"],
	"description": "",
	"content": "We will now proceed to create a Docker repository and upload a container image to this repository.\nCreate the Docker Repository Use the AWS CLI create a Docker repository on ECR which is AWS\u0026rsquo; managed container registry.\naws ecr create-repository --repository-name carla-av-demo Fetch and Upload a Docker Image We will fetch the image from the web, then import it to our newly created ECR repository.\n Fetch the docker credentials  $(aws ecr get-login --no-include-email --region us-east-1) import and tag the image  # get the repository URI ECR_REPOSITORY_URI=$(aws ecr describe-repositories --repository-names carla-av-demo --output text --query \u0026#39;repositories[0].[repositoryUri]\u0026#39;) curl https://s3.amazonaws.com/av-workshop/carla-demo.tar -o carla-demo.tar docker load -i carla-demo.tar docker tag carla-demo:latest $ECR_REPOSITORY_URI Then push the image to the ECR repository  docker push $ECR_REPOSITORY_URI:latest "
},
{
	"uri": "https://master.d2fvrafk9v089j.amplifyapp.com/02-aws-getting-started/03-aws-console-login.html",
	"title": "a. Login the AWS Console",
	"tags": ["tutorial", "aws console", "ec2"],
	"description": "",
	"content": "Depending on conditions in which you are running this guide, you may access the AWS Console through direct sign-in (here) or as directed by your trainer. To log in you will need to enter your AWS Account ID or alias, IAM user name, and Password that as provided to you for this lab.\nAfter you log in, you will see the main AWS Management Console. Take a few minutes to check it out. A search bar is located near the upper left. Recently used services are located below that. Your console may look differently depending on the recently used services. A list of all services can be found below the Services pull-down in the upper left hand corner.\nA more thorough description of the console is found here: https://aws.amazon.com/console/\nIn general, AWS documentation is found here: https://docs.aws.amazon.com/\nSupport and documentation are also found in the support pull down menu at the upper right of the management console.\nStart by selecting a region from the pull down in the upper right hand corner. For today, select the Northern Virginia region. This region is also known as us-east-1.\n"
},
{
	"uri": "https://master.d2fvrafk9v089j.amplifyapp.com/03-hpc-aws-parallelcluster-workshop/04-configure-pc.html",
	"title": "c. Create a Cluster Config",
	"tags": ["tutorial", "initialize", "ParallelCluster"],
	"description": "",
	"content": "Now that AWS ParallelCluster is installed and a default configuration has been created, you will create a configuration file to build a simple HPC system. This file will be generated in your home directory.\nPlease double check you are using the correct key-pair\n Configuring Your HPC cluster We will be generating a cluster with the following settings:\n Head-node and compute nodes will be c4.xlarge instances. Change the instance type if you like but you may run into EC2 limits that may prevent you to create some or too many instances. We use a placement group to maximize the bandwidth between instances and reduce the latency. This packs instances close together inside an Availability Zone. The cluster will have 0 compute nodes when starting, the minimum size is set to 0 and the maximum size of the cluster is set to 8 instances. We are using auto-scaling groups that will grow and shrink between the min and max limits based on the cluster utilization and job queue backlog. A GP2 Amazon EBS volume will be attached to the head-node then shared through NFS to be mounted by the compute nodes on /shared. It is generally a good location to store applications or scripts. Please keep in mind that the /home directory is shared on NFS as well. SLURM will be used as a job scheduler but there are other options you may consider in the future such as SGE. We disable Intel Hyper-threading by setting disable_hyperthreading = true in the configuration file.  For more details about the configuration options of AWS ParallelCluster please review the user guide.\n For now you can paste the following commands in your terminal:\nIFACE=$(curl --silent http://169.254.169.254/latest/meta-data/network/interfaces/macs/) SUBNET_ID=$(curl --silent http://169.254.169.254/latest/meta-data/network/interfaces/macs/${IFACE}/subnet-id) VPC_ID=$(curl --silent http://169.254.169.254/latest/meta-data/network/interfaces/macs/${IFACE}/vpc-id) AZ=$(curl http://169.254.169.254/latest/meta-data/placement/availability-zone) REGION=${AZ::-1} cd ~/environment cat \u0026gt; my-cluster-config.conf \u0026lt;\u0026lt; EOF [aws] aws_region_name = ${REGION} [global] cluster_template = default update_check = false sanity_check = true [cluster default] key_name = lab-3-your-key vpc_settings = public ebs_settings = myebs compute_instance_type = c4.xlarge master_instance_type = c4.xlarge cluster_type = ondemand placement_group = DYNAMIC placement = compute initial_queue_size = 2 max_queue_size = 8 disable_hyperthreading = true s3_read_write_resource = * scheduler = slurm [vpc public] vpc_id = ${VPC_ID} master_subnet_id = ${SUBNET_ID} [ebs myebs] shared_dir = /shared volume_type = gp2 volume_size = 20 [aliases] ssh = ssh {CFN_USER}@{MASTER_IP} {ARGS} EOF Now you are ready to launch a cluster, proceed to the next step.\n"
},
{
	"uri": "https://master.d2fvrafk9v089j.amplifyapp.com/05-aws-batch/05-setup-batch.html",
	"title": "d. Compute Environments",
	"tags": ["tutorial", "install", "AWS", "batch", "packer"],
	"description": "",
	"content": "In this part we will setup AWS Batch using the AWS Console. The AWS CLI can also be used but we will not discuss that during this workshop.\nThe steps will include the setup of a Compute Environment, a Job Queue and a Job Template.\nAccess AWS Batch  Go to Batch on your AWS console then Get Started.  You can skip the wizard when on your second screen.  Then proceed to create a Batch Compute Environment.   Create Your Compute Environment Compute environments can be seen as a computational cluster. They can consist of one or several instance kinds or just the number of cores you would like in it. For more information on the compute environments, you may want to look at this page.\nTo create a compute environment we will follow these steps:\n Select Managed Compute Environment (CE), to let AWS Batch manage the auto-scaling of EC2 resources for you. Name your Compute Environment. Let Batch create a new service Role so it can manage resources on your behalf. Let Batch create a new instance role to allow instances to call AWS APIs on your behalf. Select your EC2 key-pair (or none in our case)  Once done scroll down to configure the rest of the CE. Configure Compute Resources You will now configure your resources:\n Select On-Demand for this workshop, you can use Spot too but we can skip that for now. Choose the instance type, remove optimal and select g2.2xlarge. Set the maximum number of vCPUs to 16 which will let you use 2 instances. You can skip the rest of the settings until the AMI selection.   Select the AMI On the AMI selection screen, tick the field Enable user-specified Ami ID then use the ID of the AMI generated with Packer. Then press validate and you should see something similar to the screenshot below.\nNetwork Configuration To finalize the configuration of our CE we will do the following:\n Select the VPC, your default VPC should suffice. Select all subnets. Leave the security group as is (default SG). Add a tag called Name and as a value choose a name for your instances created with Batch. Additional tags can be added if you want it. Then click on Create to build your new Compute Environment.  At this point you have done the hard part.\n"
},
{
	"uri": "https://master.d2fvrafk9v089j.amplifyapp.com/04-amazon-fsx-for-lustre/03-lazy-loading.html",
	"title": "d. Lazy File Loading",
	"tags": ["tutorial", "HSM", "FSx", "Laxy Load"],
	"description": "",
	"content": " This section is largely inspired (or shamelessly stolen) the content of the Amazon FSx for Lustre Workshop hosted here. If interested to dive deeper we encourage your to check it out.\n As seen in the previous section, files content is not retrieved yet on the filesystem. But nothing prevents you to access your files as you would do with any other POSIX compliant file system. When accessing the file the actual content will be retrieved. The file access latency will be slightly higher for the first access but once the content has been retrieved you will encounter sub-millisecond latencies as it will be served by the file system on subsequent accesses. In addition releasing a file from the file system will not delete the file but remove it's actual content. The metadata will still be stored on the file system.\nIn this section we will dive deeper on the lazy loading functionality, observe the performances between consecutive accesses, then file content removal.\nCheck the File State We will first check the data is not loaded on Lustre for one of the files.\ntime lfs hsm_state /lustre/SEG_C3NA_Velocity.sgy You should see that the file is released, i.e. not loaded. Similar result as shown below:\nLet's check how large is the file.\ntime ls -lah /lustre/SEG_C3NA_Velocity.sgy As shown here, the file size is about 455 MB.\nRetrieving the File Content You will read the file and measure the time it takes to load it from the linked Amazon S3 bucket using the HSM. We will be writing it to tempfs.\ntime cat /lustre/SEG_C3NA_Velocity.sgy \u0026gt; /dev/shm/fsx It took about 4 seconds to retrieve the file.\nLet's run the command again and see the access time.\ntime cat /lustre/SEG_C3NA_Velocity.sgy \u0026gt; /dev/shm/fsx The new access is a bit too fast as the data has been cached on the instance so we will drop the caches and repeat our command again.\nsudo bash -c \u0026#39;echo 3 \u0026gt; /proc/sys/vm/drop_caches\u0026#39; time cat /lustre/SEG_C3NA_Velocity.sgy \u0026gt; /dev/shm/fsx This becomes a bit more realistic.\nReview the Filesystem Status Now we will look at the file content state through the HSM.\ntime lfs hsm_state /lustre/SEG_C3NA_Velocity.sgy We can see that the file state changed from released to archived.\nLet's now see how much data is stored on the Lustre partition using the command below.\ntime lfs df -h Do you notice a difference compared to the previous execution of this command? Instead of 13.5 MB of data stored we now have 468.8 MB. In this case it is stored on the second OST, your may see slightly different results.\nRelease the File Content. Now we will be releasing the file content. It will not delete nor remove the file itself. The metadata will still be stored on the MDT.\ntime sudo lfs hsm_release /lustre/SEG_C3NA_Velocity.sgy Then see again how much data is stored on our filesystem.\ntime lfs df -h And we are back to 13.5 MB of stored data.\nLet's access the file again and check how much time it takes.\ntime cat /lustre/SEG_C3NA_Velocity.sgy \u0026gt;/dev/shm/fsx It should take around 4 seconds. Subsequent reads will be using the client cache. We can drop the caches if we want too.\n"
},
{
	"uri": "https://master.d2fvrafk9v089j.amplifyapp.com/04-amazon-fsx-for-lustre/04-install-ior.html",
	"title": "d. Installing IOR",
	"tags": ["tutorial", "install", "FSx", "Performances"],
	"description": "",
	"content": "We will now conduct performance tests on our Lustre partition to evaluate the throughput it provides. For that matter, we will be using IOR an IO parallel benchmark tool used to test the performances a parallel filesystem.\nNow we will install IOR. We will use the io500-sc19 branch of the repository on the cluster head-node.\n# get IOR mkdir -p /shared/ior git clone https://github.com/hpc/ior.git cd ior git checkout io500-sc19 # load intelmpi module load intelmpi # install ./bootstrap ./configure --with-mpiio --prefix=/shared/ior make -j 10 sudo make install # set the environment export PATH=$PATH:/shared/ior/bin export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/shared/ior/lib echo \u0026#39;export PATH=$PATH:/shared/ior/bin\u0026#39; \u0026gt;\u0026gt; ~/.bashrc echo \u0026#39;export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/shared/ior/lib\u0026#39; \u0026gt;\u0026gt; ~/.bashrc Now we are ready to do some performance testing.\n"
},
{
	"uri": "https://master.d2fvrafk9v089j.amplifyapp.com/04-amazon-fsx-for-lustre/05-performance-test.html",
	"title": "e. IO Performance Testing",
	"tags": ["tutorial", "IOR", "FSx", "Performances"],
	"description": "",
	"content": "We will now run our performance tests with IOR and evaluate how much throughput can be extracted. To measure that we will be tuning several IOR parameters to favor POSIX as an IO access method and conduct direct access to the file system. This will help us to shortcut almost all caching and evaluate the raw performances Amazon FSx for Lustre can offer. Furthermore, we will generate one file per process to saturate the filesystem.\nIOR Options For this test we will use the options listed in the table below.\n   Option Description     -w Benchmark write performances.   -r Benchmark read performances.   -B Use O_DIRECT to shortcut the glibc caching layer, hit directly the OS and file system.   -o Benchmark file output path.   -a Method to use, POSIX is selected for raw perfs. MPI-IO has data caching and skew the results.   -F One file per process if present, shared file if non present. One file per process is the most intense..   -Z Changes task ordering to random ordering for readback.   -z Access is to random, not sequential, offsets within a file.   -i Number of repetitions (use 5 as a good test, stdev should be minimal).   -C Re-order tasks: change the task ordering to n+1 ordering for read-back. Avoid potential the read cache effects on clients.    The full list of options for IOR can be found here. Do not hesitate to be creative.\n Now just run the following command to test if IOR is correctly installed.\nior You should see a result similar to below. Don't be surprised by the numbers as there's a lot of caching involved.\nPerformance Tests with IOR Let's now run our performance test using multiple nodes. In the case below we will use 8 c4.xlarge instances for a total of 32 processes. Each process will write 256 MB by blocks of 64 MB. We will use the POSIX-IO method and directly access the file system to evaluate raw performances. The test will be conducted 5 times to evaluate the variance, i.e. if performances are stable, for both read and write. We don't stonewall or wait between read and writes as results will not differ.\nFirst you will generate your batch submission script by copy/pasting the code below in your AWS Cloud9 terminal. Just ensure to be connected to your cluster master instance.\n# go to your home directory cd ~ cat \u0026gt; ior_submission.sbatch \u0026lt;\u0026lt; EOF #!/bin/bash #SBATCH --job-name=ior #SBATCH --ntasks=16 #SBATCH --output=%x_%j.out mpirun /shared/ior/bin/ior -w -r -o=/lustre/test_dir -b=256m -a=POSIX -i=5 -F -z -t=64m -C EOF Then submit the script with sbatch as follows.\n# go to home again cd ~ sbatch ior_submission.sbatch The output of IOR will be written in a .out file. If you like you can use tail -f to view it as it is written. Please remember that as you will have 0 compute nodes present on your cluster up to 1 min will be necessary for instances to be created then register with the cluster. You can check the EC2 Dashboard on your AWS Console to check the status Once instances are created and registered, the IOR job will be processed.\nYou should observe a similar result as in the image below. In our case we are seeing 1 GB/s which is not too far from the 720 MB/s offered by Amazon FSx for Lustre.\nThere are other IO performance testing alternatives that you may want to try such as FIO or just a plain DD. Feel free to use them but IOR is considered as standard in HPC to evaluate parallel filesystem performances which makes easy to compare results across systems. Ever heard of the IO500 Benchmark?\n "
},
{
	"uri": "https://master.d2fvrafk9v089j.amplifyapp.com/02-aws-getting-started/04-aws-ec2-dashboard.html",
	"title": "c. AWS EC2 Dashboard",
	"tags": ["tutorial", "dashboard", "ParallelCluster"],
	"description": "",
	"content": "Next, type EC2 into the search bar, click on EC2, and move to the EC2 Dashboard. While instances are generally launched through scripts, they can also be launched directly from the EC2 dashboard. Spend a few minutes looking at the information on this page.\nResource listings and the ability to launch an instance are located in the center of the dashboard while tools and features, such as the saved Amazon Machine Images (AMI’s), Storage Volumes, and ssh keys, are accessed from the left hand side. General information is on the right hand side, such as documentation and pricing.\n"
},
{
	"uri": "https://master.d2fvrafk9v089j.amplifyapp.com/06-running-gatk-on-aws-parallelcluster/03-launch-gatk-pc.html",
	"title": "b. Build a GATK Cluster",
	"tags": ["tutorial", "create", "ParallelCluster"],
	"description": "",
	"content": "Now we will create a cluster based on the specifications defined in the configuration file. To create a cluster we will use the command pcluster create and the \u0026ndash;config (or -c) option to use another configuration file other than the default one.\nIf you were to create your cluster without using the**\u0026ndash;config** (or **-c**) option then AWS ParallelCluster will use the default configuration with the minimum requirements to get a cluster running. For example your head and compute nodes would be *t2.micro* instances instead of *m4.xlarge*.\n Run the following command in your AWS Cloud9 terminal to create a cluster. Ensure that the configuration file path is correct.\ncd ~/environment pcluster create gatk -c gatk-config.conf Your cluster will be created in a few minutes. The creation status will be displayed on your terminal. Once ready you should see a result similar to the one shown in the image below. There can be only one cluster of a given name at any time one your account.\n What's Happening in the Background When the pcluster create command is executed, AWS ParallelCluster will generate an AWS CloudFormation template to generate an infrastructure in AWS. The bulk of the work is done in AWS and once the create is launched you don't need to keep AWS ParallelCluster running. If your are interested to see AWS CloudFormation in action please follow this link. You should see something similar to what is shown in the image below.\n"
},
{
	"uri": "https://master.d2fvrafk9v089j.amplifyapp.com/03-hpc-aws-parallelcluster-workshop/06-launch-pc.html",
	"title": "d. Build an HPC Cluster",
	"tags": ["tutorial", "create", "ParallelCluster"],
	"description": "",
	"content": "Now we will create a cluster based on the specifications defined in the configuration file. To create a cluster we will use the command pcluster create and the \u0026ndash;config (or -c) option to use another configuration file other than the default one.\nIf you were to create your cluster without using the**\u0026ndash;config** (or **-c**) option then AWS ParallelCluster will use the default configuration with the minimum requirements to get a cluster running. For example your head and compute nodes would be *t2.micro* instances instead of *c4.xlarge*.\n Run the following command in your AWS Cloud9 terminal to create a cluster. Ensure that the configuration file path is correct.\npcluster create hpclab-yourname -c my-cluster-config.conf Your cluster will take a few minutes to be built. The creation status will be displayed on your terminal. Once ready you should see a result similar to the one shown in the image below. There can be only one cluster of a given name at any time one your account.\n What's Happening in the Background When the pcluster create command is executed, AWS ParallelCluster will generate an AWS CloudFormation template to generate an infrastructure in AWS. The bulk of the work is done in AWS and once the create is launched you don't need to keep AWS ParallelCluster running. If your are interested to see AWS CloudFormation in action please follow this link. You should see something similar to what is shown in the image below.\n"
},
{
	"uri": "https://master.d2fvrafk9v089j.amplifyapp.com/05-aws-batch/06-setup-batch-job-queue.html",
	"title": "e. Setup a Job Queue",
	"tags": ["tutorial", "install", "AWS", "batch", "packer"],
	"description": "",
	"content": "Now we will setup a Job Queue. This is where you will submit your jobs. Those will be dispatched to the Compute Environment(s) of your choosing by order of priority. If you are interested to know more about Job Queues, see here.\nWhen setting up a Job Queue you will need to do the following:\n Chose a name for your queue. Define a priority (1-500). This defines the priority of a Job Queue when a Compute environment is shared accross Job Queues (for example a Production Job Queue with a priority of 500 and a R\u0026amp;D Job Queue with a priority of 250). Select the Compute Environment created previously. Then create your Job Queue.  "
},
{
	"uri": "https://master.d2fvrafk9v089j.amplifyapp.com/04-amazon-fsx-for-lustre/06-cloudwatch.html",
	"title": "f. Lustre Metrics",
	"tags": ["tutorial", "IOR", "FSx", "metrics"],
	"description": "",
	"content": "Now we will visualize the metrics related to our Amazon FSx for Lustre filesystem using CloudWatch. Metrics can be used to graph several metrics such as the throughput, IOPs, create alarms or display more evolved metrics.\nIn the present case we will look at the IOPS and free space on the filesystem as shown in the image below.\nTo produce a similar result go through the following steps:\n On the AWS Console, go to the search field, search and click on Amazon FSx then select File systems (three bars on the left side). Take note of your file system ID, it should be similar to fs-0a5444e4841233. Then go to Amazon CloudWatch on the AWS Console, click on Metrics, then FSx. Select the following metrics for the file system with the same ID as noted above: FreeDataStorageCapacity, DataWriteOperations and DataReadOperations. Set the FreeDataStorageCapacity to be displayed based on the left Y Axis.  If you like you can add that graph to a dashboard or build additional metrics as discussed here. In addition, you can set alarms to send notifications on events such as low storage capacity or high bandwidth utilization. Those notifications can be used to trigger a Lambda function to create and attach a larger file system to your EC2 instances using CloudWatch Events or send you an email with informations about your environment status.\n"
},
{
	"uri": "https://master.d2fvrafk9v089j.amplifyapp.com/02-aws-getting-started/05-start_cloud9.html",
	"title": "d. Access AWS Cloud9",
	"tags": ["tutorial", "cloud9", "ParallelCluster"],
	"description": "",
	"content": "AWS Cloud9 is a cloud-based integrated development environment (IDE) that lets you write, run, and debug your code with just a browser. We will be using it to introduce you to the AWS Command Line Interface (CLI) without the need to install any software on your laptop.\nAWS Cloud9 contains a collection of tools that let you code, build, run, test, debug, and release software in the Cloud using your internet browser. The IDE offers support for python, pip, the AWS CLI and provides easy access to AWS resources through the IAM user credentials. With the IDE comes with a terminal that includes sudo privileges to the managed instance that is hosting your development environment and a pre-authenticated CLI. This makes it easy for you to quickly run commands and directly access AWS services.\nAfter accessing your IDE, please take a moment to get familiar with the Cloud9 environment. You can even take a quick tour of Cloud9 if you'd like or even watch this video.\n Create Your Own AWS Cloud9 Instance You will follow this guide if running this workshop on your own or if this the option selected by the organizers. To access your IDE you will need to go through the following steps.\n Use the AWS Console search bar by clicking on Services on the top banner to locate Cloud9 and use the search field to find it.  Click on Create Environment Name your environment with MyDevEnv and click Next Step  Use the default settings on the Configure Settings page unless told otherwise and click Next Step  Click on Create Environment You will see a review page.  Your AWS Cloud9 instance will be ready in a few minutes!\n"
},
{
	"uri": "https://master.d2fvrafk9v089j.amplifyapp.com/06-running-gatk-on-aws-parallelcluster/04-logon-gatk-pc.html",
	"title": "c. Log in to Your Cluster",
	"tags": ["tutorial", "create", "ParallelCluster"],
	"description": "",
	"content": " pcluster ssh is a wrapper around SSH. You can also log into your head-node using ssh and the public or private IP address depending on the case.\n Existing clusters can be listed using the command below. It is a convenient way to find the name of a cluster in case your forget it.\npcluster list --color Now that your cluster has been created you can log into the head-node using the following command in your AWS Cloud9 terminal:\npcluster ssh gatk -i ~/.ssh/gatk-lab-key The EC2 instance asks for confirmation of the ssh login the first time you log onto the instance. Answer with “yes”. Getting to Know Your Cluster Now that you are connected to the head-node you may want to familiarize yourself with your cluster structure by running the set of commands below.\nSLURM SLURM from SchedMD is one of the batch schedulers that you can use in AWS ParallelCluster. You will find an overview of the SLURM commands in this quickstart. The commands we will run now are the following\n  List existing partitions and nodes per partition. You should see two nodes if your run this command after creating your cluster and zero nodes if running it 10 minutes after creation (default cooldown period for AWS ParallelCluster, you don't pay for what you don't use).  sinfo  List jobs in the queues or running. Obviously there won't be any since we did not submit anything\u0026hellip;yet!  squeue NFS Shares A few volumes are shared by the head-node and will be mounted on compute instances when they boot up. You can check the list of shares using the command below. As you will see both /genomes and /home will be accessible by all nodes.\nshowmount -e localhost Next, we will run our first job!\n"
},
{
	"uri": "https://master.d2fvrafk9v089j.amplifyapp.com/03-hpc-aws-parallelcluster-workshop/07-logon-pc.html",
	"title": "e. Log in to Your Cluster",
	"tags": ["tutorial", "create", "ParallelCluster"],
	"description": "",
	"content": " pcluster ssh is a wrapper around SSH. You can also log into your head-node using ssh and the public or private IP address depending on the case.\n Existing clusters can be listed using the command below. It is a convenient way to find the name of a cluster in case your forget it.\npcluster list --color Now that your cluster has been created you can log into the head-node using the following command in your AWS Cloud9 terminal:\npcluster ssh hpclab-yourname -i ~/.ssh/lab-3-key The EC2 instance asks for confirmation of the ssh login the first time you log onto the instance. Answer with “yes”. Getting to Know your Cluster Now that you are connected to the head-node you may want to familiarize yourself with your cluster structure by running the set of commands below.\nSLURM SLURM from SchedMD is one of the batch schedulers that you can use in AWS ParallelCluster. You will find an overview of the SLURM commands in this quickstart. The commands we will run now are the following\n  List existing partitions and nodes per partition. You should see two nodes if your run this command after creating your cluster and zero nodes if running it 10 minutes after creation (default cooldown period for AWS ParallelCluster, you don't pay for what you don't use).  sinfo  List jobs in the queues or running. Obviously there won't be any since we did not submit anything\u0026hellip;yet!  squeue Module Environment Lmod is a fairly standard tool used to dynamically change your environment.\n List available modules with:  module av  Load a particular module. In this case this will load IntelMPI in your environment and check the path of mpirun.  module load intelmpi which mpirun NFS Shares A few volumes are shared by the head-node and will be mounted on compute instances when they boot up. You can check the list of shares using the command below. As you will see both /shared and /home will be accessible by all nodes.\nshowmount -e localhost Next, we will run our first job!\n"
},
{
	"uri": "https://master.d2fvrafk9v089j.amplifyapp.com/05-aws-batch/07-setup-batch-job-definition.html",
	"title": "f. Setup a Job Definition",
	"tags": ["tutorial", "install", "AWS", "batch", "packer"],
	"description": "",
	"content": "We will now setup a Job Definition, this is a template used for your jobs. While not mandatory it is a good practice to use it in order to version how your jobs are launched. For more information about Job Definitions see this documentation page.\nJob Definition Setup Go the the Job Definition screen and create a new one.\nWhen defining a job definition, you will need to do the following:\n Select a job definition name Input 5 for the number of attempts before declaring a job as failed. Input 500 for the time between attempts in seconds.  Add the job role previously defined for ECS tasks to access the output S3 bucket on your behalf. If you forgot its name use the command below in your terminal.  echo \u0026#34;ECS Job Role: $(aws cloudformation describe-stacks --stack-name PrepAVWorkshop --output text --query \u0026#39;Stacks[0].Outputs[?OutputKey == `ECSTaskPolicytoS3`].OutputValue\u0026#39;)\u0026#34; Add the container image with the repositoryUri generated when creating our ECR repository. If in doubt you can get the URI by running the command below in your terminal.  $(aws ecr describe-repositories --repository-names carla-av-demo --output text --query \u0026#39;repositories[0].[repositoryUri]\u0026#39;) Input 8 as the number of vCPUs for your job. Make the memory allocated to your container as 2000 MiB.   Environment Variable And finally, we will add an environment variable. This will tell to the application running in your container where to export data. Use the variable name EXPORT_S3_BUCKET_URL and the value corresponds to the bucket you have previously created. If you do not remember you can always run the command below in your terminal or visit the Amazon S3 dashboard on your account.\necho \u0026#34;S3 Output Bucket: $(aws cloudformation describe-stacks --stack-name PrepAVWorkshop --output text --query \u0026#39;Stacks[0].Outputs[?OutputKey == `OutputBucket`].OutputValue\u0026#39;)\u0026#34; Once all done, proceed to the creation of your Job Definition\n"
},
{
	"uri": "https://master.d2fvrafk9v089j.amplifyapp.com/04-amazon-fsx-for-lustre/07-summary.html",
	"title": "g. Summary",
	"tags": ["tutorial", "FSx", "summary"],
	"description": "",
	"content": "In this lab we learned how to create an Amazon FSx for Lustre partition with AWS ParallelCluster. We studied how lazy loading was working and conducted performances tests on the Lustre partition using IOR. Finally, we looked at the metrics of our Lustre partition using Amazon CloudWatch.\nPlease, do not forget to delete your cluster as follows:\npcluster delete my-fsx-cluster  If you are interested you can dig further on Amazon FSx for Lustre capabilities through its documentation.\n "
},
{
	"uri": "https://master.d2fvrafk9v089j.amplifyapp.com/02-aws-getting-started/06-start-aws-cli.html",
	"title": "e. Start with the AWS CLI",
	"tags": ["tutorial", "cloud9", "aws cli", "s3"],
	"description": "",
	"content": "Your AWS Cloud9 Environment should be ready. We will now get familiar with it, introduce you the AWS CLI then create an Amazon S3 bucket with the AWS CLI. This bucket will be used at the next stage.\nYour AWS Cloud9 IDE The AWS Cloud9 IDE is similar to a traditional IDE you can find on virtually any system. It is composed of a file browser, listing the files located on your instances. A series of tabs with opened files is located at the top and terminal tabs at the bottom. Tabs can be maximized to the internet browser windows size if necessary.\nUpdate the AWS CLI and Check Existing Amazon EC2 Instances Before going to the next steps we will update the AWS CLI. AWS Cloud9 should be coming with the latest version but it it always a good practice to verify you are at the latest version.. The AWS CLI allow you to manage services using the command line and, should you want it, control them through scripts. Most users will conduct some level of automation with the AWS CLI.\nEach code section contains a copy button. Use it to your advantage to quickly copy the command(s) to your clipboard.\n  Go to a Terminal page and type the command below to update the AWS CLI should update itself if necessary. You will see a warning message asking you to upgrade PIP, we won't bother to address it.  pip-3.6 install awscli -U --user Then use the commands below to display the general AWS CLI help, the help related to Amazon EC2 commands, the list of your existing instances with their key characteristics and the list of your registered SSH key-pairs. Type q to exit the help pages.  aws help aws ec2 help aws ec2 describe-instances aws ec2 describe-key-pairs Next we will use the AWS CLI to interact with S3.\nIf you are seeing a message telling you to upgrade PIP, just ignore it.\n "
},
{
	"uri": "https://master.d2fvrafk9v089j.amplifyapp.com/06-running-gatk-on-aws-parallelcluster/05-run-1stjob.html",
	"title": "d. Submit a GATK Job",
	"tags": ["tutorial", "create", "ParallelCluster"],
	"description": "",
	"content": " The steps here can also be executed on any cluster running SLURM. There may be some variations depending on your configuration but that is just to share how generic this procedure is. Except that you are doing it in the cloud.\n We will be running your first \u0026ldquo;hello world\u0026rdquo; job to introduce your to the mechanisms of AWS ParallelCluster. We will have to go through some preparatory steps before.\nCreating our GATK Script The first step is to create the scripts that we need to run GATK, and the SLURM job submission script. Here we create the GATK script command.sh.\ncat \u0026gt; command.sh \u0026lt;\u0026lt; EOF #!/bin/sh #bwa mkdir -p $1/01.bwa cd $1/01.bwa bwa mem -t 4 -R \u0026#34;@RG\\tID:group_1\\tLB:library_1\\tPL:illumina\\tSM:sample_1\u0026#34; /genomes/reference/hg38/v0/Homo_sapiens_assembly38.fasta /genomes/testdata/wgs_fastq/NA12878_20k/H06HDADXX130110.1.ATCACGAT.20k_reads_1.fastq /genomes/testdata/wgs_fastq/NA12878_20k/H06HDADXX130110.1.ATCACGAT.20k_reads_2.fastq \u0026gt; $1/01.bwa/sample.sam #samtools samtools view -b $1/01.bwa/sample.sam \u0026gt; $1/01.bwa/sample.bam samtools sort $1/01.bwa/sample.bam \u0026gt; $1/01.bwa/sample_sort.bam samtools index $1/01.bwa/sample_sort.bam #gatk mkdir -p $1/02.gatk cd $1/02.gatk gatk HaplotypeCaller -R /genomes/reference/hg38/v0/Homo_sapiens_assembly38.fasta -I $1/01.bwa/sample_sort.bam -O $1/02.gatk/sample.vcf gatk HaplotypeCaller -R /genomes/reference/hg38/v0/Homo_sapiens_assembly38.fasta -I $1/01.bwa/sample_sort.bam -O $1/02.gatk/sample.g.vcf.gz -L /genomes/testdata/intervals/wgs_calling_regions.hg38.interval_list -ERC GVCF -dbsnp /genomes/reference/hg38/v0/Homo_sapiens_assembly38.dbsnp138.vcf gatk MergeVcfs --INPUT=$1/02.gatk/sample.g.vcf.gz --OUTPUT=$1/02.gatk/sample.vcf EOF Create a Submission Script Next we create the SLURM batch submission script, run.sh. We will specify how many processes to use on the command line when we submit the job.\ncat \u0026gt; run.sh \u0026lt;\u0026lt; EOF #!/bin/sh sh command.sh /genomes/temp/1 EOF Submit a GATK Job Submitted jobs will be immediately processed if not job is in the queue and a sufficient number of compute nodes exist.\nHowever, what happens if no or not enough compute nodes can satisfy the computational requirements of the job such as the number of cores? Well, in this case AWS ParallelCluster will create new instances to satisfy the requirements of the jobs sitting in the queue. But remember that your minimum and maximum number of nodes were determined when creating the cluster. Not enough? No worries, we can update our cluster later to change those parameters.\nSo let's submit our first job as follows when on the head-node.\nsbatch -n 4 run.sh Then check the status of the queue using the command squeue. The job will be first marked as pending (PD state) because resources are being created (or in a down/drained state). If you check the EC2 Dashboard nodes should be booting up. When ready and registered your job will be processed and you will see a similar status as below.\nsqueue JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON) 2 compute he JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON) 3 compute run.sh ec2-user R 2:58 2 ip-172-31-85-230,ip-172-31-87-50 We can also check the number of nodes available in your cluster sinfo. Do not hesitate to refresh it, nodes will generally take less than 1 min to appear. In the case shown below we have one node.\nsinfo PARTITION AVAIL TIMELIMIT NODES STATE NODELIST compute* up infinite 2 alloc ip-172-31-85-230,ip-172-31-87-50 Once your job has finished, your output will be in the directory /genomes/temp/1.\nAfter a few minutes, your cluster will scale down unless there are more job to process.\n"
},
{
	"uri": "https://master.d2fvrafk9v089j.amplifyapp.com/03-hpc-aws-parallelcluster-workshop/08-run-1stjob.html",
	"title": "f. Submit your first HPC job",
	"tags": ["tutorial", "create", "ParallelCluster"],
	"description": "",
	"content": " The steps here can also be executed on any cluster running SLURM. There may be some variations depending on your configuration but that is just to share how generic this procedure is. Except that you are doing it in the cloud.\n We will be running your first \u0026ldquo;hello world\u0026rdquo; job to introduce your to the mechanisms of AWS ParallelCluster. We will have to go through some preparatory steps before.\nPreparatory Steps Please ensure that you need to go run the following commands on your AWS Cloud9 terminal and logged in the head-node.\n Creating our Hello World Application Let's first build and compile our MPI hello world application. Go back to your AWS Cloud9 terminal then run the commands below to create and build our hello world binary.\ncat \u0026gt; mpi_hello_world.c \u0026lt;\u0026lt; EOF #include \u0026lt;stdio.h\u0026gt; #include \u0026lt;stdlib.h\u0026gt; #include \u0026lt;mpi.h\u0026gt; #include \u0026lt;unistd.h\u0026gt; int main(int argc, char **argv){ int step, node, hostlen; char hostname[256]; hostlen = 255; MPI_Init(\u0026amp;argc,\u0026amp;argv); MPI_Comm_rank(MPI_COMM_WORLD, \u0026amp;node); MPI_Get_processor_name(hostname, \u0026amp;hostlen); for (step = 1; step \u0026lt; 5; step++) { printf(\u0026#34;Hello World from Step %d on Node %d, (%s)\\n\u0026#34;, step, node, hostname); sleep(2); } MPI_Finalize(); } EOF module load intelmpi mpicc mpi_hello_world.c -o mpi_hello_world If you like you can test your application locally on the head-node for testing.\nmpirun -n 4 ./mpi_hello_world Your application is compiled and ready to be executed. Let's build a batch submission script to submit it to SLURM.\nCreate a Submission Script We will create submission script that will 4 MPI processes and generate an output file for STDOUT.\ncat \u0026gt; submission_script.sbatch \u0026lt;\u0026lt; EOF #!/bin/bash #SBATCH --job-name=hello-world-job #SBATCH --ntasks=4 #SBATCH --output=%x_%j.out mpirun ./mpi_hello_world EOF Once done let's submit our fist job!\nSubmit your First Job Submitted jobs will be immediately processed if not job is in the queue and a sufficient number of compute nodes exist.\nHowever, what happens if no or not enough compute nodes can satisfy the computational requirements of the job such as the number of cores? Well, in this case AWS ParallelCluster will create new instances to satisfy the requirements of the jobs sitting in the queue. But remember that your minimum and maximum number of nodes were determined when creating the cluster. Not enough? No worries, we can update our cluster later to change those parameters.\nSo let's submit our first job as follows when on the head-node.\nsbatch submission_script.sbatch Then check the status of the queue using the command squeue. The job will be first marked as pending (PD state) because resources are being created (or in a down/drained state). If you check the EC2 Dashboard nodes should be booting up. When ready and registered your job will be processed and you will see a similar status as below.\nJOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON) 2 compute hello-wo ec2-user R 0:05 1 ip-10-0-1-154 We can also check the number of nodes available in your cluster sinfo. Do not hesitate to refresh it, nodes will generally take less than 1 min to appear. In the case shown below we have one node.\nsinfo PARTITION AVAIL TIMELIMIT NODES STATE NODELIST compute* up infinite 1 alloc ip-10-0-1-154 Once the job has been processed you should see a similar results as here in the .out file:\nHello World from Step 1 on Node 1 (ip-10-0-1-154) Hello World from Step 1 on Node 2 (ip-10-0-1-154) Hello World from Step 1 on Node 3 (ip-10-0-1-154) Hello World from Step 1 on Node 0 (ip-10-0-1-154) ... Hello World from Step 4 on Node 1 (ip-10-0-1-154) Hello World from Step 4 on Node 2 (ip-10-0-1-154) Hello World from Step 4 on Node 0 (ip-10-0-1-154) Hello World from Step 4 on Node 3 (ip-10-0-1-154) Voila, it is easy, isn't it !\nAfter a few minutes, your cluster will scale down unless there are more job to process. We will now take a look at what is really happening behind the curtain.\n"
},
{
	"uri": "https://master.d2fvrafk9v089j.amplifyapp.com/05-aws-batch/08-describe-batch-env.html",
	"title": "g. Describe Your Environment",
	"tags": ["tutorial", "install", "AWS", "batch", "packer"],
	"description": "",
	"content": "Now what we configured Batch, let's take a look at what we have with the following commands\naws batch describe-compute-environments aws batch describe-job-queues aws batch describe-job-definitions You will see that the JSONs provided as output contain the parameters you filled for the Compute Environment, Job Queue and Job Definition. Please keep in mind that everything we did in the previous step using the AWS Console can also be done with the AWS CLI, AWS SDK or CloudFormation.\n"
},
{
	"uri": "https://master.d2fvrafk9v089j.amplifyapp.com/02-aws-getting-started/07-work-with-s3.html",
	"title": "f. Using Amazon S3",
	"tags": ["tutorial", "cloud9", "s3"],
	"description": "",
	"content": "Now that we have access to the AWS CLI, we will use it to create an S3 bucket then upload a file to this bucket. While those steps could be done using the AWS Console, they will give you a good idea of what can be done with the AWS CLI. In addition if interested, you can find more details about S3 on the related documentation page.\nEvery bucket must have a unique name. If this is not the case Amazon S3 will not allow you to create one.\n  Go to an AWS Cloud9 terminal and list existing buckets with the command below. You should see several to no buckets.  aws s3 ls Make an Amazon S3 Bucket with the command shown below. Please keep in mind that bucket names should be unique so either use a random prefix or postfix or append your name. The bucket name must start by s3://.  BUCKET_POSTFIX=$(uuidgen --random | cut -d\u0026#39;-\u0026#39; -f1) aws s3 mb s3://bucket-${BUCKET_POSTFIX} cat \u0026lt;\u0026lt; EOF ***** Take Note of Your Bucket Name ***** Bucket Name = bucket-${BUCKET_POSTFIX} ***************************************** EOF Download a file from the internet to your AWS Cloud9 instance, for example lets retrieve a synthetic subsurface model generally used to test Seismic Imaging algorithms. The file will be downloaded on your AWS Cloud9 Instance, not your computer.  wget http://s3.amazonaws.com/open.source.geoscience/open_data/seg_eage_salt/SEG_C3NA_Velocity.sgy Upload the file to your Amazon S3 bucket using  aws s3 cp ./SEG_C3NA_Velocity.sgy s3://bucket-${BUCKET_POSTFIX}/SEG_C3NA_Velocity.sgy List the content of your bucket using the command below, alternatively you can use the AWS Console and select the S3 Dashboard the go into your newly created bucket to see the file.  aws s3 ls s3://bucket-${BUCKET_POSTFIX}/ Once done, you can delete the local version of the file using the command rm or the AWS Cloud9 IDE interface.  rm SEG_C3NA_Velocity.sgy See the process in Cloud9 (click on the image to increase its size). And the result in the AWS Console: Please keep note of your bucket name as it has been randomly generated, see the Amazon S3 Dashboard if you are unsure in the AWS console\n "
},
{
	"uri": "https://master.d2fvrafk9v089j.amplifyapp.com/03-hpc-aws-parallelcluster-workshop/09-behind-the-curtain.html",
	"title": "g. Behind the Curtain",
	"tags": ["tutorial", "create", "ParallelCluster"],
	"description": "",
	"content": "Let's now look at what really happens in AWS when you submit a job and reveal a bit of the magic behind your auto-scaling computational resources.\nAt the heart of AWS ParallelCluster exists an auto-scaling group. It is a logical group of instances that can scale up and scale down based on a series of criteria. In the case of AWS ParallelCluster we have three processes controlling the scaling of the cluster. These processes will:\n Check the queue for any pending job and compute the total number of instances needed to process the queued jobs. Check if instances are busy over a cooldown period (600 seconds or 10 mins) and emit a heartbeat if they are not doing anything. If no jobs are waiting in the queue and some instances are sitting idle then they will be terminated and the auto-scaling group reduced.  More details can be found in the documentation of AWS ParallelCluster.\nLet's check in more details how this works.\nAuto-scaling Groups We will now take a look at auto-scaling groups\n Open the AWS Console, select the EC2 Dashboard, then go to Auto Scaling Groups at the bottom left. You should see an auto-scaling group with Desired at 0, Min at 0 and Max at 8. The Min and Max value correspond to the min_queue_size and max_queue_size of your cluster configuration file.  Go back to your AWS Cloud9 environment and launch a new job with the commands below. It will just run a wait for 5 minutes.  cat \u0026gt; sleep_script.sbatch \u0026lt;\u0026lt; EOF #!/bin/bash #SBATCH --job-name=hello-world-job #SBATCH --ntasks=2 #SBATCH --output=%x_%j.out sleep 300 EOF sbatch sleep_script.sbatch Go back to the EC2 Dashboard and Auto Scaling Groups, refresh the fields with the circling arrows if necessary. You should see that an instance just appeared on the desired field instead of 0. It corresponds to the 2 physical cores or c4.xlarge equivalent that we requested.  On the EC2 Dashboard click on Instances on the left side of the window. You should see your compute instances labeled as Compute.   Now you have a better understanding on how AWS ParallelCluster operates. If you are interested, please look at the documentation as there is more to explore.\n"
},
{
	"uri": "https://master.d2fvrafk9v089j.amplifyapp.com/05-aws-batch/09-run-job.html",
	"title": "g. Run a Single Job",
	"tags": ["tutorial", "install", "AWS", "batch", "packer"],
	"description": "",
	"content": "On this part we will launch jobs using the AWS CLI. The AWS Console and the AWS SDK can be used as well but we won't discuss that here. We will run a single job with the AWS CLI as show below. Please replace the Job Queue name and Job Definition with the ones you just created.\naws batch submit-job --job-name my-job --job-queue YOUR-JOB-QUEUE-NAME --job-definition YOUR-JOB-DEFINITION-NAME  If something goes wrong here you need to double check the queue name and job definition.\n Keep track of the JobId as we will now use it to show the status of a job:\naws batch describe-jobs --jobs YOUR-JOB-ID A JSON will be shown and describe the status of you job.\n"
},
{
	"uri": "https://master.d2fvrafk9v089j.amplifyapp.com/02-aws-getting-started/08-start-ec2.html",
	"title": "g. Create an EC2 Instance",
	"tags": ["tutorial", "cloud9", "aws cli", "ec2", "s3"],
	"description": "",
	"content": "Now that you are getting familiar with the AWS Console and AWS CLI we will raise the bar and do the following: create an SSH key-pair on you AWS Cloud9 instance, create an instance then access it.\nGenerate an SSH Key-pair SSH is commonly used to connect to Amazon EC2 instances. To allow you to connect to your instances we will generate a key-pair using the AWS CLI on your AWS Cloud9 instance as shown below. We will use the key name mykey but feel free to choose yours.\naws ec2 create-key-pair --key-name lab-2-your-key --query KeyMaterial --output text \u0026gt; ~/.ssh/id_rsa chmod 600 ~/.ssh/id_rsa If you like, you can check if your key is registered using the command:\naws ec2 describe-key-pairs Create a New Amazon EC2 Instance We will now create a new EC2 instance but first we will need to check into which Virtual Private Cloud (or VPC) we will place it. We will select the same VPC as our AWS Cloud9 instance and ideally the same subnet.\n Grab the Subnet ID and VPC ID from the Cloud 9 instance, we'll use this in the next step to launch an instance:  MAC=$(curl -s http://169.254.169.254/latest/meta-data/network/interfaces/macs/) cat \u0026lt;\u0026lt; EOF *********************************************************************************** Subnet ID = $(curl -s http://169.254.169.254/latest/meta-data/network/interfaces/macs/$MAC/subnet-id) VPC ID = $(curl -s http://169.254.169.254/latest/meta-data/network/interfaces/macs/$MAC/vpc-id) ************************************************************************************ EOF Go to the Amazon EC2 Dashboard then to the Instances section Click on the blue Launch Instance button.  Select the Amazon Linux 2 AMI and click Select. Choose a t2.micro instance then click Next: Configure Instance Details. On the Network section select the VPC ID and same Subnet ID from your AWS Cloud9 Instance. click on Next: Add Storage and leave the Storage section as is, click Next: Add Tags. Click Add Tag then as a Key input Name (literally, not your name!). As Value add [Your Name]\u0026lsquo;s Instance or any significant name. This will be displayed as the name of your instance. Then click on Next: Configure Security Groups.  Select the tick Create a new security group, change the Security Group name if you want. The type should be ssh, protocol TCP, port range 22 and the source 0.0.0.0/0. Click Review and Launch. Ignore the warnings should you see any. You will see a review page with the instance specifications. Click on Launch then select the lab-2-your-key key-pair created earlier.   Your instance is being launched and ready in a few seconds, go to the Instances section of the EC2 Dashboard to check its state.\nConnect to Your Instance Do you have issues to connect to your instance? Access the AWS Console and the EC2 Dashboard. Select your instance and review its details.\n Once the instance is running, go back to your AWS Cloud9 Environment, access a terminal and go through the following steps:\n List running instances and display their names, type, private IP address and public IP address. Here the information will be filtered to only keep certain details (hence the complex command). The same information will be displayed on the EC2 Dashboard.  aws ec2 describe-instances --query \u0026#39;Reservations[*].Instances[*].[Tags[?Key==`Name`]| [0].Value,InstanceType, PrivateIpAddress, PublicIpAddress]\u0026#39; --filters Name=instance-state-name,Values=running --output table Connect to your instances with SSH using either the public or private IP address and the username ec2-user which is the default use for Amazon Linux. Type yes when asked if you want to connect to the instance.  # don\u0026#39;t forget to use your OWN ip address # keep the username to ec2-user as is, don\u0026#39;t use your name! ssh ec2-user@10.0.1.6 Ping the internet to test the outbound connectivity.  ping www.wikipedia.org  Do not forget to change the IP address of the instance you are trying to connect to! This must be yours not the one you are seeing above!\n You now have an functional instance that can communicate with the outside world! Go to the next section to see what else we can do.\nIf you cannot connect to your instances, please check that the security groups are set correctly. If needed, go to the EC2 Dashboard, Instances, select your instances and review the inbound and outbound rules. If you need to modify the security groups, select your instance, click on Actions, then select Networking then click on Security Groups. In addition, you may want to check that you are using the correct key-pair.\n "
},
{
	"uri": "https://master.d2fvrafk9v089j.amplifyapp.com/06-running-gatk-on-aws-parallelcluster/06-behind-the-curtain.html",
	"title": "e. Terminate Your Cluster",
	"tags": ["tutorial", "create", "ParallelCluster"],
	"description": "",
	"content": " Note: If you plan on working on the next labs then you will want to clean up the ParallelCluster configuration created during that one. Files stored in the bucket and on cloud9 will incur small charges (less than a dollar a month if the tutorial was followed as written.) The cloud9 IDE will stop (not terminate) 30 minutes after the web tab is closed and can be started again, through the console, when needed.\n Now that we are done with our cluster we can delete it using the command below from your AWS Cloud9 terminal.\npcluster delete gatk The cluster and all its resources will be deleted by CloudFormation. If you like you can check the CloudFormation Dashboard in your AWS Console to how the deletion processes is conducted.\n"
},
{
	"uri": "https://master.d2fvrafk9v089j.amplifyapp.com/03-hpc-aws-parallelcluster-workshop/10-delete-pc.html",
	"title": "h. Terminate Your Cluster",
	"tags": ["tutorial", "create", "ParallelCluster"],
	"description": "",
	"content": " Note: If you plan on working on the next labs then you will want to clean up the ParallelCluster configuration created during that one. Files stored in the bucket and on cloud9 will incur small charges (less than a dollar a month if the tutorial was followed as written.) The cloud9 IDE will stop (not terminate) 30 minutes after the web tab is closed and can be started again, through the console, when needed.\n Now that we are done with our cluster we can delete it using the command below from your AWS Cloud9 terminal.\npcluster delete hpclab-yourname The cluster and all its resources will be deleted by CloudFormation. If you like you can check the CloudFormation Dashboard in your AWS Console to how the deletion processes is conducted.\n"
},
{
	"uri": "https://master.d2fvrafk9v089j.amplifyapp.com/05-aws-batch/10-run-array-job.html",
	"title": "i. Run an Array Job",
	"tags": ["tutorial", "install", "AWS", "batch", "packer"],
	"description": "",
	"content": "Now we will run an Array Job, it is a way to submit many jobs at once. This time we will create a JSON file that will contain the parameters of the job array.\nFirst, we create the JSON file by modifying the text below to reflect your environment properties then pasting it to your terminal. As before, please change the Job Queue name and Job Definition Name to reflect the ones you will be using.\ncat \u0026lt;\u0026lt;EOF \u0026gt; ./my-job.json { \u0026#34;jobName\u0026#34;: \u0026#34;my-job\u0026#34;, \u0026#34;jobQueue\u0026#34;: \u0026#34;YOUR-JOB-QUEUE-NAME\u0026#34;, \u0026#34;arrayProperties\u0026#34;: { \u0026#34;size\u0026#34;: 500 }, \u0026#34;jobDefinition\u0026#34;: \u0026#34;YOUR-JOB-DEFINITION-NAME\u0026#34; } EOF Now that we created our job parameters, we can submit our array job as follows:\naws batch submit-job --cli-input-json file://my-job.json We just launched an array job of 500 jobs. To know more about job arrays see the documentation on Array Jobs\n"
},
{
	"uri": "https://master.d2fvrafk9v089j.amplifyapp.com/02-aws-getting-started/09-ec2-s3-iam.html",
	"title": "h. IAM and Roles",
	"tags": ["tutorial", "cloud9", "aws cli", "ec2", "iam"],
	"description": "",
	"content": " You will encounter an issue now! But that's normal and we will explain why.\n Access to resources by users and services is controlled by Identity and Access Management (IAM). For example, IAM permissions can be added to a policy then a role which can be attached to a user user, group role (admins, devops) or a service (Amazon EC2 to access Amazon S3, Amazon Lambda to access Amazon SQS). For an in depth introduction, please review the IAM Documentation\n Now that you created an EC2 instance and logged into it using SSH, we will get instance to access the Amazon S3 bucket created previously.\naws s3 ls s3://bucket-${BUCKET_POSTFIX}/ It appears that your instances has not been granted permission to access your Amazon S3 storage. In the present, your AWS Cloud9 has been granted permissions to some services. However, your new Amazon EC2 instance has not been given any permissions and we will now grant it access to S3.\nCreate an IAM Role for Amazon EC2 We will now create a role so your Amazon EC2 instance can access your S3 bucket.\n On the AWS Console, click on Services on the top bar. Search and open the IAM Dashboard, click on Roles on the left side of the Dashboard, then click on the blue Create Role button.  Select AWS Service in Select type of trusted entity, then chose EC2 in Choose the service that will use this role, then click on Next: Permissions. In the search field input S3 and click on the box next to the policy AmazonS3FullAccess to provide full Amazon S3 access for Amazon EC2 instance. Then click Next: Tags. Leave the tags as is then click Next: Review. Input a Role Name such as S3FullAccessForEC2 then click Create Role.  Your role is now created. Search for your role S3FullAccessForEC2. Click on its name to take a detailed look at the new role and policy. Take a first look at the Trust Relationships tab and you will see that Amazon EC2 is one of the trusted policies (meaning it can use this role).\nThe take a look at the Permissions tab and expand the AmazonS3FullAccess policy then select {}JSON. You should see the permissions below. These are quite open as they allow your Amazon EC2 instances to conduct any action on Amazon S3. However, we could also restrict these permissions for only some actions, such as List or Put, to be conducted on a particular Amazon S3 bucket.\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;s3:*\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } However, we could also restrict these permissions for only some actions, such as List or Put, to be conducted on a particular Amazon S3 bucket. For example:\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;s3:Get*\u0026#34;, \u0026#34;s3:List*\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::bucket-myname\u0026#34;, \u0026#34;arn:aws:s3:::bucket-myname/*\u0026#34; ] } ] }  Please note that full access to Amazon S3 is fine in the context of this workshop but fine grained control is highly recommended for anything else than temporary sandbox testing.\n IAM is a great way to control who and what can access to which resources at a fine level of granularity. Please see this page If you are want to know more about IAM policies and the IAM Policy Simulator.\nAttach the IAM Role to an Amazon EC2 Instance Now that you have created a new IAM role, we will assign it to our EC2 instance:\n Go to the EC2 Dashboard then Instances. Select your instance, then click on Actions, select Instance Settings and Attach/Replace IAM Role. Select the newly created IAM Role, click Apply Your role should be attached and visible on your EC2 instance details and it is now allowed to access S3.  Go back to your AWS Cloud9 IDE, connect back to the instance with SSH and run the following commands (don't forget to change the bucket name to yours!). This will list your Amazon S3 bucket content then download the file downloaded previously.  aws s3 ls s3://bucket-${BUCKET_POSTFIX}/ aws s3 cp s3://bucket-${BUCKET_POSTFIX}/SEG_C3NA_Velocity.sgy . ls -l If everything went right you should see the a result similar to the one shown in the image below. "
},
{
	"uri": "https://master.d2fvrafk9v089j.amplifyapp.com/05-aws-batch/11-optional.html",
	"title": "j. What do do next?",
	"tags": ["tutorial", "install", "AWS", "batch", "optional"],
	"description": "",
	"content": "There are many things you could do next such as: generate videos of from the simulations, add lambda functions to do that for you or send you notifications once jobs are processed.\nGenerate Videos from Simulations The results of the simulations will be exported in your S3 bucket as zip files, you can fetch them then view the RGB, segmentation and depth images. Go to your S3 bucket directly through the AWS Console or using the following codes.\nS3_BUCKET=$(aws cloudformation describe-stacks --stack-name $STACK_NAME --output text --query \u0026#39;Stacks[0].Outputs[?OutputKey == `OutputBucket`].OutputValue\u0026#39;) echo \u0026#39;Your S3 bucket is: $S3_BUCKET\u0026#39; You can then list the content of your bucket using the following command:\naws s3 ls s3://$S3_BUCKET And when ready copy one of the files to your AWS Cloud9 IDE (or desktop) using the command below. Just don't forget to replace the_archive_to_retrieve by a real archive name.\naws s3 cp s3://$S3_BUCKET/the_archive_to_retrieve.tar.gz . Once an archive fetched and expanded you can build a movie from the images using the following code. We will let you take care of the installation of ffmpeg yourself.\nconvert -quality 100 depth/*.png depth.mp4 convert -quality 100 rgb/*.png rgb.mp4 convert -quality 100 seg/*.png seg.mp4 ffmpeg -i depth.mp4 -i seg.mp4 -i rgb.mp4 -filter_complex \u0026#34;[0:v][1:v][2:v]hstack=3\u0026#34; -c:v libx264 -crf 0 output.mp4 What Else Could You Do? As next steps you could do the followings:\n Setup notifications on job or instance state change using Amazon CloudWatch Events to trigger an email or SMS notification using Amazon SNS. You could also do the same to add events to a Amazon DynamoDB table using an Amazon Lambda function. Or use any other example found on this page.  And please, Let us know what cool things you are building.\n"
},
{
	"uri": "https://master.d2fvrafk9v089j.amplifyapp.com/04-amazon-fsx-for-lustre.html",
	"title": "Amazon FSx for Lustre",
	"tags": ["HPC", "Overview", "Batch"],
	"description": "",
	"content": "Overview Amazon FSx for Lustre provides a high-performance file system optimized for fast processing of workloads such as machine learning, high performance computing (HPC), video processing, financial modeling, and electronic design automation (EDA). These workloads commonly require data to be presented via a fast and scalable file system interface, and typically have data sets stored on long-term data stores like Amazon S3.\nAmazon FSx for Lustre allows you to build a Lustre file system based with an S3 backend. Files can be transferred between the Lustre partition and Amazon S3 using the Lustre Hierarchical Storage Management (HSM). The size of your file system will determine how much throughput will be provided by Amazon FSx for Lustre, for more details on that topic see the performance page of this service.\nOne key advantage of Amazon FSx for Lustre is that you can create a Lustre partition based on the required size or throughput you need. If linked to an Amazon S3 bucket, the size of the Lustre partition can be lesser than the total size of the Amazon S3 bucket. Indeed, when created, FSx for Lustre will copy the metadata from the objects stored in the bucket but the actual content or bytes of the files will not be retrieved until needed.\nFor this lab one will assume that you have an AWS Cloud9 IDE ready. If this is not the case, please run through the first half of Getting Started with AWS lab. It will also be assumed that you have minimal AWS ParallelCluster knowledge or went through the previous lab.\n In this lab we will introduce you to Amazon FSx for Lustre using AWS ParallelCluster and will drive your through the following steps:\n Upload files from an AWS Cloud9 instance to an S3 bucket. Create a new cluster with AWS ParallelCluster and Amazon FSx for Lustre. Run an IO intensive application to test Lustre performances. Push and get files using Lustre HSM. Delete the cluster and the Lustre partition.  "
},
{
	"uri": "https://master.d2fvrafk9v089j.amplifyapp.com/05-aws-batch.html",
	"title": "AWS Batch",
	"tags": ["HPC", "Overview", "Batch"],
	"description": "",
	"content": "Overview For this lab one will assume that you have an AWS Cloud9 IDE ready. If this is not the case, please run through the first half of Getting Started with AWS lab.\n In this lab, you will learn how to run a driving simulation with CARLA that will output sensor data to be used in later stages of a workflow. CARLA is an interactive driving simulator used to support development, training, and validation of autonomous driving systems.\nThis lab, you will learn a series of steps :\n Setup the infrastructure for Batch. Upload a container with the simulation. Build an AMI containing CARLA and the ECS agent. Run simulations on Batch.  "
},
{
	"uri": "https://master.d2fvrafk9v089j.amplifyapp.com/02-aws-getting-started/99-summary.html",
	"title": "i. Summary",
	"tags": ["tutorial", "summary"],
	"description": "",
	"content": "During this tutorial you have acquired the basic knowledge on how to build an infrastructure in AWS and in particular how to:\n Access and use the AWS Console. Use the AWS CLI access and create AWS resources. Create an Amazon EC2 instance. Define a new IAM role and attach it to an Amazon EC2 instance.  You will see that building an HPC system in AWS is not much different that what you have done so far. Simple isn't it?\nIn the next section you will learn how to build a HPC cluster using AWS ParallelCluster. Let's get started.\n"
},
{
	"uri": "https://master.d2fvrafk9v089j.amplifyapp.com/",
	"title": "AWS HPC Workshops",
	"tags": [],
	"description": "",
	"content": "Welcome to the Amazon HPC Workshop website Amazon Web Services provides the most elastic and scalable cloud infrastructure to run your HPC applications. With virtually unlimited capacity, engineers, researchers, and HPC system owners can innovate beyond the limitations of on-premises HPC infrastructure.\nAWS delivers an integrated suite of services that provides everything needed to quickly and easily build and manage HPC clusters in the cloud to run the most compute intensive workloads across various industry verticals.\nThese workloads span the traditional HPC applications, like genomics, computational chemistry, financial risk modeling, computer aided engineering, weather prediction, and seismic imaging, as well as emerging applications, like machine learning, deep learning, and autonomous driving.\nHPC on AWS removes the long wait times and lost productivity often associated with on-premises HPC clusters. Flexible configuration and virtually unlimited scalability allow you to grow and shrink your infrastructure as your workloads dictate, not the other way around. Additionally, with access to a broad portfolio of cloud-based services like Data Analytics, Artificial Intelligence (AI), and Machine Learning (ML), you can redefine traditional HPC workflows to innovate faster.\nToday, more cloud-based HPC applications run on AWS than on any other cloud. Customers like Bristol-Myers Squibb, FINRA, BP and Autodesk trust AWS to run their most critical HPC workloads.\nSelect a workshop from the left panel or just click and explore the workshops highlighted below.\n"
},
{
	"uri": "https://master.d2fvrafk9v089j.amplifyapp.com/tags/aws.html",
	"title": "AWS",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://master.d2fvrafk9v089j.amplifyapp.com/tags/aws-console.html",
	"title": "aws console",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://master.d2fvrafk9v089j.amplifyapp.com/tags/batch.html",
	"title": "Batch",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://master.d2fvrafk9v089j.amplifyapp.com/tags/configuration.html",
	"title": "configuration",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://master.d2fvrafk9v089j.amplifyapp.com/tags/configure.html",
	"title": "configure",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://master.d2fvrafk9v089j.amplifyapp.com/tags/create.html",
	"title": "create",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://master.d2fvrafk9v089j.amplifyapp.com/tags/ec2.html",
	"title": "ec2",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://master.d2fvrafk9v089j.amplifyapp.com/tags/fsx.html",
	"title": "FSx",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://master.d2fvrafk9v089j.amplifyapp.com/tags/hsm.html",
	"title": "HSM",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://master.d2fvrafk9v089j.amplifyapp.com/tags/initialize.html",
	"title": "initialize",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://master.d2fvrafk9v089j.amplifyapp.com/tags/install.html",
	"title": "install",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://master.d2fvrafk9v089j.amplifyapp.com/tags/ior.html",
	"title": "IOR",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://master.d2fvrafk9v089j.amplifyapp.com/tags/laxy-load.html",
	"title": "Laxy Load",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://master.d2fvrafk9v089j.amplifyapp.com/tags/metrics.html",
	"title": "metrics",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://master.d2fvrafk9v089j.amplifyapp.com/tags/optional.html",
	"title": "optional",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://master.d2fvrafk9v089j.amplifyapp.com/tags/packer.html",
	"title": "packer",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://master.d2fvrafk9v089j.amplifyapp.com/tags/parallelcluster.html",
	"title": "ParallelCluster",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://master.d2fvrafk9v089j.amplifyapp.com/tags/performances.html",
	"title": "Performances",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://master.d2fvrafk9v089j.amplifyapp.com/tags/pre-requisite.html",
	"title": "Pre-Requisite",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://master.d2fvrafk9v089j.amplifyapp.com/tags/summary.html",
	"title": "summary",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://master.d2fvrafk9v089j.amplifyapp.com/tags.html",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://master.d2fvrafk9v089j.amplifyapp.com/tags/tutorial.html",
	"title": "tutorial",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://master.d2fvrafk9v089j.amplifyapp.com/authors.html",
	"title": "",
	"tags": [],
	"description": "",
	"content": "body { max-width: 980px; border: 1px solid #ddd; outline: 1300px solid #fff; margin: 16px auto; } body .markdown-body { padding: 45px; } @font-face { font-family: fontawesome-mini; src: url(data:font/woff;charset=utf-8;base64,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) format('woff'); } .markdown-body { font-family: sans-serif; -ms-text-size-adjust: 100%; -webkit-text-size-adjust: 100%; color: #333333; overflow: hidden; font-family: \"Helvetica Neue\", Helvetica, \"Segoe UI\", Arial, freesans, sans-serif; font-size: 16px; line-height: 1.6; word-wrap: break-word; } .markdown-body a { background: transparent; } .markdown-body a:active, .markdown-body a:hover { outline: 0; } .markdown-body b, .markdown-body strong { font-weight: bold; } .markdown-body mark { background: #ff0; color: #000; font-style: italic; font-weight: bold; } .markdown-body sub, .markdown-body sup { font-size: 75%; line-height: 0; position: relative; vertical-align: baseline; } .markdown-body sup { top: -0.5em; } .markdown-body sub { bottom: -0.25em; } .markdown-body h1 { font-size: 2em; margin: 0.67em 0; } .markdown-body img { border: 0; } .markdown-body hr { -moz-box-sizing: content-box; box-sizing: content-box; height: 0; } .markdown-body pre { overflow: auto; } .markdown-body code, .markdown-body kbd, .markdown-body pre, .markdown-body samp { font-family: monospace, monospace; font-size: 1em; } .markdown-body input { color: inherit; font: inherit; margin: 0; } .markdown-body html input[disabled] { cursor: default; } .markdown-body input { line-height: normal; } .markdown-body input[type=\"checkbox\"] { box-sizing: border-box; padding: 0; } .markdown-body table { border-collapse: collapse; border-spacing: 0; } .markdown-body td, .markdown-body th { padding: 0; } .markdown-body .codehilitetable { border: 0; border-spacing: 0; } .markdown-body .codehilitetable tr { border: 0; } .markdown-body .codehilitetable pre, .markdown-body .codehilitetable div.codehilite { margin: 0; } .markdown-body .linenos, .markdown-body .code, .markdown-body .codehilitetable td { border: 0; padding: 0; } .markdown-body td:not(.linenos) .linenodiv { padding: 0 !important; } .markdown-body .code { width: 100%; } .markdown-body .linenos div pre, .markdown-body .linenodiv pre, .markdown-body .linenodiv { border: 0; -webkit-border-radius: 0; -moz-border-radius: 0; border-radius: 0; -webkit-border-top-left-radius: 3px; -webkit-border-bottom-left-radius: 3px; -moz-border-radius-topleft: 3px; -moz-border-radius-bottomleft: 3px; border-top-left-radius: 3px; border-bottom-left-radius: 3px; } .markdown-body .code div pre, .markdown-body .code div { border: 0; -webkit-border-radius: 0; -moz-border-radius: 0; border-radius: 0; -webkit-border-top-right-radius: 3px; -webkit-border-bottom-right-radius: 3px; -moz-border-radius-topright: 3px; -moz-border-radius-bottomright: 3px; border-top-right-radius: 3px; border-bottom-right-radius: 3px; } .markdown-body * { -moz-box-sizing: border-box; box-sizing: border-box; } .markdown-body input { font: 13px Helvetica, arial, freesans, clean, sans-serif, \"Segoe UI Emoji\", \"Segoe UI Symbol\"; line-height: 1.4; } .markdown-body a { color: #4183c4; text-decoration: none; } .markdown-body a:hover, .markdown-body a:focus, .markdown-body a:active { text-decoration: underline; } .markdown-body hr { height: 0; margin: 15px 0; overflow: hidden; background: transparent; border: 0; border-bottom: 1px solid #ddd; } .markdown-body hr:before, .markdown-body hr:after { display: table; content: \" \"; } .markdown-body hr:after { clear: both; } .markdown-body h1, .markdown-body h2, .markdown-body h3, .markdown-body h4, .markdown-body h5, .markdown-body h6 { margin-top: 15px; margin-bottom: 15px; line-height: 1.1; } .markdown-body h1 { font-size: 30px; } .markdown-body h2 { font-size: 21px; } .markdown-body h3 { font-size: 16px; } .markdown-body h4 { font-size: 14px; } .markdown-body h5 { font-size: 12px; } .markdown-body h6 { font-size: 11px; } .markdown-body blockquote { margin: 0; } .markdown-body ul, .markdown-body ol { padding: 0; margin-top: 0; margin-bottom: 0; } .markdown-body ol ol, .markdown-body ul ol { list-style-type: lower-roman; } .markdown-body ul ul ol, .markdown-body ul ol ol, .markdown-body ol ul ol, .markdown-body ol ol ol { list-style-type: lower-alpha; } .markdown-body dd { margin-left: 0; } .markdown-body code, .markdown-body pre, .markdown-body samp { font-family: Consolas, \"Liberation Mono\", Menlo, Courier, monospace; font-size: 12px; } .markdown-body pre { margin-top: 0; margin-bottom: 0; } .markdown-body kbd { background-color: #e7e7e7; background-image: -moz-linear-gradient(#fefefe, #e7e7e7); background-image: -webkit-linear-gradient(#fefefe, #e7e7e7); background-image: linear-gradient(#fefefe, #e7e7e7); background-repeat: repeat-x; border-radius: 2px; border: 1px solid #cfcfcf; color: #000; padding: 3px 5px; line-height: 10px; font: 11px Consolas, \"Liberation Mono\", Menlo, Courier, monospace; display: inline-block; } .markdown-body*:first-child { margin-top: 0 !important; } .markdown-body*:last-child { margin-bottom: 0 !important; } .markdown-body .headerlink { font: normal 400 16px fontawesome-mini; vertical-align: middle; margin-left: -16px; float: left; display: inline-block; text-decoration: none; opacity: 0; color: #333; } .markdown-body .headerlink:focus { outline: none; } .markdown-body h1 .headerlink { margin-top: 0.8rem; } .markdown-body h2 .headerlink, .markdown-body h3 .headerlink { margin-top: 0.6rem; } .markdown-body h4 .headerlink { margin-top: 0.2rem; } .markdown-body h5 .headerlink, .markdown-body h6 .headerlink { margin-top: 0; } .markdown-body .headerlink:hover, .markdown-body h1:hover .headerlink, .markdown-body h2:hover .headerlink, .markdown-body h3:hover .headerlink, .markdown-body h4:hover .headerlink, .markdown-body h5:hover .headerlink, .markdown-body h6:hover .headerlink { opacity: 1; text-decoration: none; } .markdown-body h1 { padding-bottom: 0.3em; font-size: 2.25em; line-height: 1.2; border-bottom: 1px solid #eee; } .markdown-body h2 { padding-bottom: 0.3em; font-size: 1.75em; line-height: 1.225; border-bottom: 1px solid #eee; } .markdown-body h3 { font-size: 1.5em; line-height: 1.43; } .markdown-body h4 { font-size: 1.25em; } .markdown-body h5 { font-size: 1em; } .markdown-body h6 { font-size: 1em; color: #777; } .markdown-body p, .markdown-body blockquote, .markdown-body ul, .markdown-body ol, .markdown-body dl, .markdown-body table, .markdown-body pre, .markdown-body .admonition { margin-top: 0; margin-bottom: 16px; } .markdown-body hr { height: 4px; padding: 0; margin: 16px 0; background-color: #e7e7e7; border: 0 none; } .markdown-body ul, .markdown-body ol { padding-left: 2em; } .markdown-body ul ul, .markdown-body ul ol, .markdown-body ol ol, .markdown-body ol ul { margin-top: 0; margin-bottom: 0; } .markdown-body lip { margin-top: 16px; } .markdown-body dl { padding: 0; } .markdown-body dl dt { padding: 0; margin-top: 16px; font-size: 1em; font-style: italic; font-weight: bold; } .markdown-body dl dd { padding: 0 16px; margin-bottom: 16px; } .markdown-body blockquote { padding: 0 15px; color: #777; border-left: 4px solid #ddd; } .markdown-body blockquote:first-child { margin-top: 0; } .markdown-body blockquote:last-child { margin-bottom: 0; } .markdown-body table { display: block; width: 100%; overflow: auto; word-break: normal; word-break: keep-all; } .markdown-body table th { font-weight: bold; } .markdown-body table th, .markdown-body table td { padding: 6px 13px; border: 1px solid #ddd; } .markdown-body table tr { background-color: #fff; border-top: 1px solid #ccc; } .markdown-body table tr:nth-child(2n) { background-color: #f8f8f8; } .markdown-body img { max-width: 100%; -moz-box-sizing: border-box; box-sizing: border-box; } .markdown-body code, .markdown-body samp { padding: 0; padding-top: 0.2em; padding-bottom: 0.2em; margin: 0; font-size: 85%; background-color: rgba(0,0,0,0.04); border-radius: 3px; } .markdown-body code:before, .markdown-body code:after { letter-spacing: -0.2em; content: \"\\00a0\"; } .markdown-body precode { padding: 0; margin: 0; font-size: 100%; word-break: normal; white-space: pre; background: transparent; border: 0; } .markdown-body .codehilite { margin-bottom: 16px; } .markdown-body .codehilite pre, .markdown-body pre { padding: 16px; overflow: auto; font-size: 85%; line-height: 1.45; background-color: #f7f7f7; border-radius: 3px; } .markdown-body .codehilite pre { margin-bottom: 0; word-break: normal; } .markdown-body pre { word-wrap: normal; } .markdown-body pre code { display: inline; max-width: initial; padding: 0; margin: 0; overflow: initial; line-height: inherit; word-wrap: normal; background-color: transparent; border: 0; } .markdown-body pre code:before, .markdown-body pre code:after { content: normal; } /* Admonition */ .markdown-body .admonition { -webkit-border-radius: 3px; -moz-border-radius: 3px; position: relative; border-radius: 3px; border: 1px solid #e0e0e0; border-left: 6px solid #333; padding: 10px 10px 10px 30px; } .markdown-body .admonition table { color: #333; } .markdown-body .admonition p { padding: 0; } .markdown-body .admonition-title { font-weight: bold; margin: 0; } .markdown-body .admonition.admonition-title { color: #333; } .markdown-body .attention.admonition-title { color: #a6d796; } .markdown-body .caution.admonition-title { color: #d7a796; } .markdown-body .hint.admonition-title { color: #96c6d7; } .markdown-body .danger.admonition-title { color: #c25f77; } .markdown-body .question.admonition-title { color: #96a6d7; } .markdown-body .note.admonition-title { color: #d7c896; } .markdown-body .admonition:before, .markdown-body .attention:before, .markdown-body .caution:before, .markdown-body .hint:before, .markdown-body .danger:before, .markdown-body .question:before, .markdown-body .note:before { font: normal normal 16px fontawesome-mini; -moz-osx-font-smoothing: grayscale; -webkit-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; line-height: 1.5; color: #333; position: absolute; left: 0; top: 0; padding-top: 10px; padding-left: 10px; } .markdown-body .admonition:before { content: \"\\f056\\00a0\"; color: 333; } .markdown-body .attention:before { content: \"\\f058\\00a0\"; color: #a6d796; } .markdown-body .caution:before { content: \"\\f06a\\00a0\"; color: #d7a796; } .markdown-body .hint:before { content: \"\\f05a\\00a0\"; color: #96c6d7; } .markdown-body .danger:before { content: \"\\f057\\00a0\"; color: #c25f77; } .markdown-body .question:before { content: \"\\f059\\00a0\"; color: #96a6d7; } .markdown-body .note:before { content: \"\\f040\\00a0\"; color: #d7c896; } .markdown-body .admonition::after { content: normal; } .markdown-body .attention { border-left: 6px solid #a6d796; } .markdown-body .caution { border-left: 6px solid #d7a796; } .markdown-body .hint { border-left: 6px solid #96c6d7; } .markdown-body .danger { border-left: 6px solid #c25f77; } .markdown-body .question { border-left: 6px solid #96a6d7; } .markdown-body .note { border-left: 6px solid #d7c896; } .markdown-body .admonition*:first-child { margin-top: 0 !important; } .markdown-body .admonition*:last-child { margin-bottom: 0 !important; } /* progress bar*/ .markdown-body .progress { display: block; width: 300px; margin: 10px 0; height: 24px; -webkit-border-radius: 3px; -moz-border-radius: 3px; border-radius: 3px; background-color: #ededed; position: relative; box-shadow: inset -1px 1px 3px rgba(0, 0, 0, .1); } .markdown-body .progress-label { position: absolute; text-align: center; font-weight: bold; width: 100%; margin: 0; line-height: 24px; color: #333; text-shadow: 1px 1px 0 #fefefe, -1px -1px 0 #fefefe, -1px 1px 0 #fefefe, 1px -1px 0 #fefefe, 0 1px 0 #fefefe, 0 -1px 0 #fefefe, 1px 0 0 #fefefe, -1px 0 0 #fefefe, 1px 1px 2px #000; -webkit-font-smoothing: antialiased !important; white-space: nowrap; overflow: hidden; } .markdown-body .progress-bar { height: 24px; float: left; -webkit-border-radius: 3px; -moz-border-radius: 3px; border-radius: 3px; background-color: #96c6d7; box-shadow: inset 0 1px 0 rgba(255, 255, 255, .5), inset 0 -1px 0 rgba(0, 0, 0, .1); background-size: 30px 30px; background-image: -webkit-linear-gradient( 135deg, rgba(255, 255, 255, .4) 27%, transparent 27%, transparent 52%, rgba(255, 255, 255, .4) 52%, rgba(255, 255, 255, .4) 77%, transparent 77%, transparent ); background-image: -moz-linear-gradient( 135deg, rgba(255, 255, 255, .4) 27%, transparent 27%, transparent 52%, rgba(255, 255, 255, .4) 52%, rgba(255, 255, 255, .4) 77%, transparent 77%, transparent ); background-image: -ms-linear-gradient( 135deg, rgba(255, 255, 255, .4) 27%, transparent 27%, transparent 52%, rgba(255, 255, 255, .4) 52%, rgba(255, 255, 255, .4) 77%, transparent 77%, transparent ); background-image: -o-linear-gradient( 135deg, rgba(255, 255, 255, .4) 27%, transparent 27%, transparent 52%, rgba(255, 255, 255, .4) 52%, rgba(255, 255, 255, .4) 77%, transparent 77%, transparent ); background-image: linear-gradient( 135deg, rgba(255, 255, 255, .4) 27%, transparent 27%, transparent 52%, rgba(255, 255, 255, .4) 52%, rgba(255, 255, 255, .4) 77%, transparent 77%, transparent ); } .markdown-body .progress-100plus .progress-bar { background-color: #a6d796; } .markdown-body .progress-80plus .progress-bar { background-color: #c6d796; } .markdown-body .progress-60plus .progress-bar { background-color: #d7c896; } .markdown-body .progress-40plus .progress-bar { background-color: #d7a796; } .markdown-body .progress-20plus .progress-bar { background-color: #d796a6; } .markdown-body .progress-0plus .progress-bar { background-color: #c25f77; } .markdown-body .candystripe-animate .progress-bar{ -webkit-animation: animate-stripes 3s linear infinite; -moz-animation: animate-stripes 3s linear infinite; animation: animate-stripes 3s linear infinite; } @-webkit-keyframes animate-stripes { 0% { background-position: 0 0; } 100% { background-position: 60px 0; } } @-moz-keyframes animate-stripes { 0% { background-position: 0 0; } 100% { background-position: 60px 0; } } @keyframes animate-stripes { 0% { background-position: 0 0; } 100% { background-position: 60px 0; } } .markdown-body .gloss .progress-bar { box-shadow: inset 0 4px 12px rgba(255, 255, 255, .7), inset 0 -12px 0 rgba(0, 0, 0, .05); } /* MultiMarkdown Critic Blocks */ .markdown-body .critic_mark { background: #ff0; } .markdown-body .critic_delete { color: #c82829; text-decoration: line-through; } .markdown-body .critic_insert { color: #718c00 ; text-decoration: underline; } .markdown-body .critic_comment { color: #8e908c; font-style: italic; } .markdown-body .headeranchor { font: normal normal 16px fontawesome-mini; line-height: 1; display: inline-block; text-decoration: none; -webkit-font-smoothing: antialiased; -moz-osx-font-smoothing: grayscale; -webkit-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; } .headeranchor:before { content: '\\e157'; } .markdown-body .task-list-item { list-style-type: none; } .markdown-body .task-list-item+.task-list-item { margin-top: 3px; } .markdown-body .task-list-item input { margin: 0 4px 0.25em -20px; vertical-align: middle; } /* Media */ @media only screen and (min-width: 480px) { .markdown-body { font-size:14px; } } @media only screen and (min-width: 768px) { .markdown-body { font-size:16px; } } @media print { .markdown-body * { background: transparent !important; color: black !important; filter:none !important; -ms-filter: none !important; } .markdown-body { font-size:12pt; max-width:100%; outline:none; border: 0; } .markdown-body a, .markdown-body a:visited { text-decoration: underline; } .markdown-body .headeranchor-link { display: none; } .markdown-body a[href]:after { content: \" (\" attr(href) \")\"; } .markdown-body abbr[title]:after { content: \" (\" attr(title) \")\"; } .markdown-body .ir a:after, .markdown-body a[href^=\"javascript:\"]:after, .markdown-body a[href^=\"#\"]:after { content: \"\"; } .markdown-body pre { white-space: pre; white-space: pre-wrap; word-wrap: break-word; } .markdown-body pre, .markdown-body blockquote { border: 1px solid #999; padding-right: 1em; page-break-inside: avoid; } .markdown-body .progress, .markdown-body .progress-bar { -moz-box-shadow: none; -webkit-box-shadow: none; box-shadow: none; } .markdown-body .progress { border: 1px solid #ddd; } .markdown-body .progress-bar { height: 22px; border-right: 1px solid #ddd; } .markdown-body tr, .markdown-body img { page-break-inside: avoid; } .markdown-body img { max-width: 100% !important; } .markdown-body p, .markdown-body h2, .markdown-body h3 { orphans: 3; widows: 3; } .markdown-body h2, .markdown-body h3 { page-break-after: avoid; } } /*GitHub*/ .codehilite {background-color:#fff;color:#333333;} .codehilite .hll {background-color:#ffffcc;} .codehilite .c{color:#999988;font-style:italic} .codehilite .err{color:#a61717;background-color:#e3d2d2} .codehilite .k{font-weight:bold} .codehilite .o{font-weight:bold} .codehilite .cm{color:#999988;font-style:italic} .codehilite .cp{color:#999999;font-weight:bold} .codehilite .c1{color:#999988;font-style:italic} .codehilite .cs{color:#999999;font-weight:bold;font-style:italic} .codehilite .gd{color:#000000;background-color:#ffdddd} .codehilite .ge{font-style:italic} .codehilite .gr{color:#aa0000} .codehilite .gh{color:#999999} .codehilite .gi{color:#000000;background-color:#ddffdd} .codehilite .go{color:#888888} .codehilite .gp{color:#555555} .codehilite .gs{font-weight:bold} .codehilite .gu{color:#800080;font-weight:bold} .codehilite .gt{color:#aa0000} .codehilite .kc{font-weight:bold} .codehilite .kd{font-weight:bold} .codehilite .kn{font-weight:bold} .codehilite .kp{font-weight:bold} .codehilite .kr{font-weight:bold} .codehilite .kt{color:#445588;font-weight:bold} .codehilite .m{color:#009999} .codehilite .s{color:#dd1144} .codehilite .n{color:#333333} .codehilite .na{color:teal} .codehilite .nb{color:#0086b3} .codehilite .nc{color:#445588;font-weight:bold} .codehilite .no{color:teal} .codehilite .ni{color:purple} .codehilite .ne{color:#990000;font-weight:bold} .codehilite .nf{color:#990000;font-weight:bold} .codehilite .nn{color:#555555} .codehilite .nt{color:navy} .codehilite .nv{color:teal} .codehilite .ow{font-weight:bold} .codehilite .w{color:#bbbbbb} .codehilite .mf{color:#009999} .codehilite .mh{color:#009999} .codehilite .mi{color:#009999} .codehilite .mo{color:#009999} .codehilite .sb{color:#dd1144} .codehilite .sc{color:#dd1144} .codehilite .sd{color:#dd1144} .codehilite .s2{color:#dd1144} .codehilite .se{color:#dd1144} .codehilite .sh{color:#dd1144} .codehilite .si{color:#dd1144} .codehilite .sx{color:#dd1144} .codehilite .sr{color:#009926} .codehilite .s1{color:#dd1144} .codehilite .ss{color:#990073} .codehilite .bp{color:#999999} .codehilite .vc{color:teal} .codehilite .vg{color:teal} .codehilite .vi{color:teal} .codehilite .il{color:#009999} .codehilite .gc{color:#999;background-color:#EAF2F5} Credits   Project Contributors  Linda Hedges Anh Tran Pierre-Yves  "
},
{
	"uri": "https://master.d2fvrafk9v089j.amplifyapp.com/tags/aws-cli.html",
	"title": "aws cli",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://master.d2fvrafk9v089j.amplifyapp.com/categories.html",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://master.d2fvrafk9v089j.amplifyapp.com/tags/cloud9.html",
	"title": "cloud9",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://master.d2fvrafk9v089j.amplifyapp.com/authors.html",
	"title": "Credits",
	"tags": [],
	"description": "",
	"content": "\u0026lt;br\u0026gt;The HPC AWS ParallelCluster tutorial is largely based on Linda's great document ! \u0026lt;br\u0026gt; \u0026lt;br\u0026gt;Linda Hedges - \u0026lt;a href = \u0026quot;mailto: hedgesl@amazon.com\u0026quot;\u0026gt;Email\u0026lt;/a\u0026gt; \u0026lt;br\u0026gt; \u0026lt;br\u0026gt;Anh Tran \u0026lt;a href = \u0026quot;mailto: trhn@amazon.com\u0026quot;\u0026gt;Email\u0026lt;/a\u0026gt; \u0026lt;br\u0026gt; \u0026lt;br\u0026gt;Pierre-Yves Aquilanti \u0026lt;a href = \u0026quot;mailto: pierreya@amazon.com\u0026quot;\u0026gt;Email\u0026lt;/a\u0026gt;  "
},
{
	"uri": "https://master.d2fvrafk9v089j.amplifyapp.com/tags/dashboard.html",
	"title": "dashboard",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://master.d2fvrafk9v089j.amplifyapp.com/tags/iam.html",
	"title": "iam",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://master.d2fvrafk9v089j.amplifyapp.com/tags/s3.html",
	"title": "s3",
	"tags": [],
	"description": "",
	"content": ""
}]